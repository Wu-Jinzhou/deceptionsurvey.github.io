<!DOCTYPE html>
<html class="fontawesome-i2svg-active fontawesome-i2svg-complete">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<style data-merge-styles="true"></style>
	<style data-merge-styles="true"></style>
	<style data-merge-styles="true"></style>
	<meta name="description" content="AI Deception Survey">
	<meta name="keywords" content="AI Deception, Survey, AI Safety, AI Alignment">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>AI Deception Survey</title>
	<link rel="icon" href="./assets/logo_transp.png" type="image/png">
	<!-- å¼•å…¥cssæ–‡ä»¶ -->
	<link href="./assets/css" rel="stylesheet">
	<link rel="stylesheet" href="./assets/bulma.min.css">
	<link rel="stylesheet" href="./assets/bulma-carousel.min.css">
	<link rel="stylesheet" href="./assets/font-face.css">
	<link rel="stylesheet" href="./assets/bulma-slider.min.css">
	<link rel="stylesheet" href="./assets/fontawesome.all.min.css">
	<link rel="stylesheet" href="./assets/academicons.min.css">
	<link rel="stylesheet" href="./assets/index.css">
	<link rel="stylesheet" href="./assets/leaderboard.css">
	<style>
		p, ul {
		text-align: justify;
		margin-left: auto;
		margin-right: auto;
		width: 75%;
		list-style:disc;
		margin-bottom: 10px;
		}
		li {
		margin-bottom: 10px;
		}
		
		.section-card {
		background-color: #f8f9fa;
		border: 1px solid #e1e5e9;
		border-radius: 12px;
		padding: 25px;
		margin: 25px 0;
		box-shadow: 0 4px 8px rgba(0,0,0,0.1);
		transition: transform 0.3s ease, box-shadow 0.3s ease;
		}
		
		.section-card:hover {
		transform: translateY(-5px);
		box-shadow: 0 8px 16px rgba(0,0,0,0.15);
		}
		
		.section-card h3 {
		color: #2c3e50;
		margin-bottom: 15px;
		}
		
		.section-card h2 {
		color: #2c3e50;
		margin-bottom: 20px;
		font-size: 2.5em;
		font-weight: bold;
		text-align: center;
		}
		
		.pdf-link {
		display: inline-block;
		background-color: #3498db;
		color: white;
		padding: 8px 16px;
		border-radius: 4px;
		text-decoration: none;
		margin-top: 10px;
		transition: background-color 0.3s ease;
		}
		
		.pdf-link:hover {
		background-color: #2980b9;
		color: white;
		}
	</style>
</head>

<nav2 class="navbar" role="navigation" aria-label="main navigation">
	<div class="navbar-brand">
		<a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
			<span aria-hidden="true"></span>
			<span aria-hidden="true"></span>
			<span aria-hidden="true"></span>
		</a>
	</div>
	<div class="navbar-menu">
		<div class="navbar-start" style="flex-grow: 1; justify-content: center;">
			<div class="navbar-item has-dropdown is-hoverable">
				<a class="navbar-link">
					<span class="icon">
						<svg xmlns="http://www.w3.org/2000/svg" xml:space="preserve" width="512" height="501" shape-rendering="geometricPrecision" text-rendering="geometricPrecision" image-rendering="optimizeQuality" fill-rule="evenodd" clip-rule="evenodd" viewBox="0 0 512 500.59">
							<path fill-rule="nonzero"
								d="m107.22 313.44-23.58 55.2a7.338 7.338 0 0 1-6.57 4.47l-59.32 5.33 45.22 39.48c2.1 1.84 2.9 4.62 2.32 7.17l-13.36 58.52 51.51-30.8c2.4-1.44 5.3-1.34 7.54 0l51.53 30.8-5.87-25.66c4.88.55 9.91.19 14.8-1.15l6.68 29.31c.19 1.27.19 2.58-.02 3.89a12.8 12.8 0 0 1-1.62 4.47 12.559 12.559 0 0 1-7.71 5.73c-3.1.78-6.5.37-9.49-1.41l-52.07-31.13-51.62 30.88c-1.12.74-2.38 1.29-3.73 1.63l-.94.23c-1.45.26-2.99.26-4.55-.04l-.5-.09c-3.34-.78-6.08-2.85-7.78-5.56a12.573 12.573 0 0 1-1.67-9.24l13.57-59.39-45.68-39.89a12.566 12.566 0 0 1-2.94-3.78 12.6 12.6 0 0 1-1.29-4.2l-.05-.65c-.22-3.31.91-6.45 2.94-8.87 2.01-2.39 4.9-4.05 8.2-4.42l60.67-5.41 23.78-55.71c.62-1.47 1.54-2.85 2.7-4.02.94-.95 2.04-1.76 3.3-2.39.44-.24.91-.44 1.4-.6 2.96-1.06 6.12-.91 8.87.19 2.75 1.11 5.12 3.19 6.54 6.01l18.82 44.01-5.47 23.93c-.4-.5-.72-1.05-.97-1.64l-23.59-55.2zm137.19-302.2c0-6.2 5.03-11.24 11.24-11.24 6.2 0 11.24 5.04 11.24 11.24V62c0 6.2-5.04 11.24-11.24 11.24-6.21 0-11.24-5.04-11.24-11.24V11.24zM40.94 151.78c-5.7-2.4-8.38-8.96-5.98-14.66 2.4-5.7 8.97-8.38 14.67-5.98l46.73 19.8c5.7 2.4 8.38 8.97 5.98 14.66-2.4 5.7-8.97 8.38-14.67 5.98l-46.73-19.8zm47.52-72.84c-4.36-4.39-4.34-11.49.05-15.85 4.39-4.36 11.49-4.34 15.85.05l35.7 36.08c4.36 4.39 4.34 11.49-.05 15.85-4.39 4.36-11.49 4.34-15.85-.05l-35.7-36.08zm71.65-49.34c-2.35-5.72.38-12.27 6.1-14.62 5.73-2.35 12.27.38 14.63 6.1l19.3 46.95c2.35 5.72-.39 12.27-6.11 14.62-5.72 2.35-12.27-.38-14.62-6.1l-19.3-46.95zm302.26 101.54c5.7-2.4 12.27.28 14.67 5.98 2.4 5.7-.28 12.26-5.98 14.66l-46.73 19.8c-5.7 2.4-12.27-.28-14.67-5.98-2.4-5.69.28-12.26 5.98-14.66l46.73-19.8zm-54.73-68c4.36-4.39 11.46-4.41 15.85-.05 4.39 4.36 4.41 11.46.05 15.85l-35.7 36.08c-4.36 4.39-11.46 4.41-15.85.05-4.39-4.36-4.41-11.46-.05-15.85l35.7-36.08zm-76.48-42.06c2.36-5.72 8.9-8.45 14.63-6.1 5.72 2.35 8.45 8.9 6.1 14.62l-19.3 46.95c-2.35 5.72-8.9 8.45-14.62 6.1-5.72-2.35-8.46-8.9-6.11-14.62l19.3-46.95zm-53.09 110.8 33.5 78.45 83.93 7.53c.77 0 1.53.06 2.27.16 6.17.76 11.54 3.87 15.3 8.34l1.22 1.67c2.97 4.22 4.6 9.39 4.32 14.82l-.2 2.03c-.37 2.69-1.19 5.23-2.36 7.54l-.93 1.6c-.84 1.36-1.81 2.62-2.89 3.79-.82.97-1.76 1.85-2.79 2.6l-63.56 55.5 19.33 84.72c1.09 6.02-.21 11.99-3.28 16.87a23.685 23.685 0 0 1-14.82 10.51c-3.12.74-6.32.78-9.41.23l-.01.06c-3.19-.57-6.23-1.79-8.91-3.54L256 381.22l-74.01 44.25a23.716 23.716 0 0 1-17.57 2.45c-5.92-1.49-11.25-5.23-14.61-10.81l-.9-1.71a23.7 23.7 0 0 1-2.18-6.9c-.48-3.03-.37-6.08.29-8.96l19.1-83.63-64.6-56.4a23.796 23.796 0 0 1-8.09-16.36c-.39-6 1.52-12.21 5.85-17.16a23.87 23.87 0 0 1 7.47-5.73c2.81-1.4 5.93-2.19 9.12-2.37l84.56-7.56 33.69-78.88a23.734 23.734 0 0 1 13.02-12.75c12.18-4.91 25.86 1.03 30.83 12.93l.1.25zM416.44 303.3l23.72 55.56 60.67 5.41c3.3.37 6.19 2.03 8.2 4.42 2.03 2.42 3.16 5.56 2.94 8.87l-.05.65a12.6 12.6 0 0 1-1.29 4.2c-.72 1.43-1.73 2.72-2.94 3.78l-45.68 39.89 13.57 59.39c.68 3.29 0 6.58-1.67 9.24-1.7 2.71-4.44 4.78-7.78 5.56l-.5.09c-1.56.3-3.1.3-4.55.04l-.94-.23c-1.35-.34-2.61-.89-3.73-1.63l-51.63-30.88-52.06 31.13a12.513 12.513 0 0 1-9.49 1.41c-3.1-.78-5.93-2.76-7.71-5.73-.84-1.41-1.38-2.93-1.63-4.45l-.01-.08c-.21-1.25-.2-2.54 0-3.81l6.69-29.36c1.09.3 2.21.55 3.33.75 3.75.68 7.62.84 11.46.41l-5.87 25.68 51.52-30.8c2.25-1.34 5.14-1.44 7.54 0l51.52 30.8-13.37-58.52c-.57-2.55.23-5.33 2.33-7.17l45.22-39.48-59.32-5.33a7.338 7.338 0 0 1-6.57-4.47l-23.58-55.2-23.57 55.2c-.26.61-.61 1.18-1.02 1.69l-5.46-23.91 18.84-44.08c1.42-2.82 3.79-4.9 6.54-6.01 2.75-1.1 5.91-1.25 8.87-.19.49.16.96.36 1.4.6 1.26.63 2.36 1.44 3.3 2.39 1.16 1.17 2.08 2.55 2.7 4.02l.06.15z" />
							<path fill="#F7D345" fill-rule="nonzero"
								d="m263.04 137.82 37.45 87.7 95.01 8.52c4.19.37 7.29 4.06 6.93 8.25a7.59 7.59 0 0 1-2.57 5.07v.01l-71.87 62.75 21.26 93.02c.94 4.11-1.63 8.2-5.74 9.15-2.1.47-4.2.04-5.88-1.06L256 362.43l-81.89 48.96a7.627 7.627 0 0 1-10.46-2.64 7.602 7.602 0 0 1-.89-5.61h-.01l21.26-93.02-71.87-62.75c-3.17-2.77-3.49-7.59-.72-10.76a7.621 7.621 0 0 1 5.33-2.59l94.75-8.5 37.48-87.75c1.65-3.88 6.13-5.68 10.01-4.03 1.91.81 3.31 2.31 4.05 4.08z" />
						</svg>
					</span> &nbsp;&nbsp; More Research </a>
				<div class="navbar-dropdown">
					<a class="navbar-item" href="https://github.com/PKU-Alignment/safe-rlhf">
						<b>SafeRLHF (ICLR 2024 Spotlight)</b>
					</a>
					<a class="navbar-item" href="https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF">
						<b>PKU-SafeRLHF (ACL2025 Main)</b>
					</a>
					<a class="navbar-item" href="https://github.com/PKU-Alignment/aligner">
						<b>Aligner (NeurIPS 2024 Oral)</b>
					</a>
					<a class="navbar-item" href="https://github.com/PKU-Alignment/safe-sora">
						<b>SafeSora (NeurIPS 2024 DB Track)</b>
					</a>
					<a class="navbar-item" href="https://github.com/PKU-Alignment/beavertails">
						<b>BeaverTails (NeurIPS 2023 DB Track)</b>
					</a>
					<a class="navbar-item" href="https://align-anything.readthedocs.io/en/latest/index.html">
						<b>Align-AnythingðŸ”¥ðŸ”¥ðŸ”¥</b>
					</a>
					<a class="navbar-item" href="https://alignmentsurvey.com">
						<b>AI Alignment: A Comprehensive Survey</b>
					</a>
				</div>
			</div>
		</div>
	</div>
</nav2>

<section class="hero" style="background-color: #B22222;">
	<div class="hero-body">
		<div class="container is-max-desktop">
			<div class="columns is-centered">
				<div class="column has-text-centered">
					<h1 class="title is-1 publication-title" style="color: white;">
						AI Deception: A Comprehensive Survey
					</h1>
					<div class="is-size-5 publication-authors">
						<span class="author-block" style="color: white;">PKU-Alignment Team</span>
					</div>
					<div class="is-size-5 publication-authors">
						<span class="author-block" style="color: white;">
							<em>Peking University</em>
						</span>
					</div>
					<br>
					<div class="column has-text-centered">
						<div class="publication-links">
							<!-- PDF Link. -->
							<span class="link-block">
								<a href="paper.pdf" class="external-link button is-normal is-rounded is-dark">
									<span class="icon">
										<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg="" style="width: 1.2em; height: 1.2em;">
											<path fill="currentColor"
												d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z">
											</path>
										</svg>
									</span>
									<span>Paper</span>
								</a>
							</span>
						</div>
					</div>
				</div>
			</div>
		</div>
	</div>
</section>

<br>

<section class="section">
	<div class="container is-max-desktop">
		<div class="content has-text-centered">
			<h2 style="text-align: center;">Overview</h2>
			<p>Recent advancements have highlighted the practical impact of AI systems across a wide spectrum of applications. However, the enhanced capabilities have raised increasing safety concerns, as AI deception behaviors are increasingly observed in both research and application. <b>AI deception can be broadly defined as behaviors by AI systems that induces false beliefs in humans or other AI systems, thereby securing outcomes that are advantageous to the AI itself.</b> This survey aims to synthesize and systematize existing research on AI deception, with a focus on the genesis and mitigations of AI deception.</p>
			<p>This survey aims to synthesize and systematize existing research on AI deception spanning RL agents, language models, and prospective superintelligent systems. In this survey, we introduce the <b>concept of AI deception</b>, <b>typologies</b>, <b>underlying mechanisms</b>, and <b>potential solutions to AI deception</b>, and discuss open challenges and future research directions.</p>
		</div>
	</div>
</section>

<section class="section">
	<div class="container is-max-desktop">
		
		<!-- AI Deception Cycle -->
		<div class="section-card">
			<h2>AI Deception Cycle</h2>
			<div style="text-align: center; margin: 20px 0;">
				<img src="./images/ai_deception_cycle.png" alt="AI Deception Cycle" style="max-width: 90%; height: auto; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
			</div>
			<p>This section provides a placeholder description of the AI deception cycle, exploring the fundamental mechanisms and patterns through which artificial intelligence systems may engage in deceptive behaviors. The content examines the genesis and evolution of deceptive capabilities in AI systems, including triggering conditions, feedback loops, and escalation patterns.</p>
		</div>

		<!-- Empirical Studies -->
		<div class="section-card">
			<h2>Empirical Studies</h2>
			<div style="text-align: center; margin: 20px 0;">
				<img src="./images/empirical_studies.png" alt="Empirical Studies" style="max-width: 90%; height: auto; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
			</div>
			<p>Taxonomy of AI deception across three classes: <i>Behavioral-Signaling Deception</i>, <i>Internal Process Deception</i>, and <i>Goal-Environment Deception</i>. AI deceptions are mapped along dimensions of oversight vigilance and detection difficulty, showing progression from overt behavioral signals to covert environmental manipulation.</p>
		</div>

		<!-- Typologies and Risks of Deception -->
		<div class="section-card">
			<h2>Risks of Deception</h2>
			<div style="text-align: center; margin: 20px 0;">
				<img src="./images/typologies_risks.png" alt="Typologies and Risks of Deception" style="max-width: 90%; height: auto; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
			</div>
			<p style="text-align: justify; margin-left: auto; margin-right: auto; width: 75%;">
				The risks associated with AI deception span a spectrum, from subtle misrepresentations that distort usersâ€™ perceptions of the world, to more serious cases where deception is used to gain user privileges, and ultimately to situations involving severe harm and systemic risk. We introduce a five-level progressive taxonomy of AI deception, grounded in the scope of deceptive behavior, the challenges of oversight, and the extent of its impact.
			</p>
			<ul style="text-align: justify; margin-left: auto; margin-right: auto; width: 75%;">
				<li>
					<b>R1: Cognitive Misleading</b> &mdash; AI systems emit deceptive signals that locally distort usersâ€™ beliefs or expectations. These deceptive behaviours often stem from anthropomorphic cues or misleading user interface outputs and are generally easy to detect and mitigate.
				</li>
				<li>
					<b>R2: Strategic Manipulation</b> &mdash; During prolonged or repeated interactions, AI agents may employ deceptive strategies to guide users toward actions that benefit the system. These behaviors often lack clear warning signs and are hard to detect in real time.
				</li>
				<li>
					<b>R3: Objective Misgeneralization</b> &mdash; AI systems may exploit flaws in their training objectives, leading to behaviors that appear competent but misaligned with human intent. Such reward hacking or specification gaming introduces deceptive outputs not through explicit intent, but as a byproduct of optimizing poorly specified goalsâ€”posing significant risks when these behaviors transfer undetected to deployment settings.
				</li>
				<li>
					<b>R4: Institutional Erosion</b> &mdash; At scale, deceptive AI systems can erode trust in scientific institutions, governance frameworks, and coordination mechanisms. These risks underscore broader societal consequences of large-scale deployment, including the erosion of epistemic norms and the decline of institutional credibility.
				</li>
				<li>
					<b>R5: Capability Concealment with Runaway Potential</b> &mdash; In extreme cases, advanced systems may conceal their capabilities or objectives to evade detection or shutdown, thereby enabling the pursuit of long-term goals. This form of AI deception may escape human oversight and trigger an irreversible loss of control.
				</li>
			</ul>
			<p style="text-align: justify; margin-left: auto; margin-right: auto; width: 75%;">
				Each level introduces qualitatively distinct failure modes, and mitigating lower-level risks does not necessarily prevent higher-level ones. Moreover, seemingly benign deceptive behaviors at lower levels can escalate into more serious threats as their scope expands.
			</p>
		</div>
		<!-- Incentive Foundations -->
		<div class="section-card">
			<h2>Incentive Foundations</h2>
			<div style="text-align: center; margin: 20px 0;">
				<img src="./images/incentive_foundations.png" alt="Incentive Foundations" style="max-width: 90%; height: auto; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
			</div>
			<p>As the training stage progresses, root causes of emergent deception arise sequentially as the progressive framework. Before training, <b>data contamination</b> occurs when preparing training data; <b>reward misspecification</b> occurs when designing the training procedure; they collectively form the seed of deceptive strategies. During the training, due to <b>goal misgeneralization</b>, deceptive strategies are internalized and stabilized into instrumental goals. Later in deployment, these goals can motivate more sophisticated, long-term deception with higher complexity and risk.
			</p>
		</div>

		<!-- Capability Preconditions -->
		<div class="section-card">
			<h2>Capability Preconditions</h2>
			<div style="text-align: center; margin: 20px 0;">
				<img src="./images/capability_preconditions.png" alt="Capability Preconditions" style="max-width: 90%; height: auto; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
			</div>
			<p>The emergence of deceptive behaviors in AI systems is closely tied to specific capabilities that enable models to recognize opportunities for deception, plan deceptive strategies, and execute them effectively. 
				<b>Perception:</b> capabilities for understanding the world and self, including high-level abilities (situational awareness, theory of mind) and foundational ones (world modeling, self-knowledge). 
				<b>Planning:</b> capabilities for strategic thinking, with goal-directedness as the high-level driver supported by strategic reasoning and long-term planning. 
				<b>Performing:</b> capabilities for implementing deception, e.g. tool-use and persuasion.
				These capabilities are not mutually exclusive and can be combined in various ways to create deceptive strategies.
			</p>
		</div>

		<!-- Contextual Triggers -->
		<div class="section-card">
			<h2>Contextual Triggers</h2>
			<div style="text-align: center; margin: 20px 0;">
				<img src="./images/contextual_triggers.png" alt="Contextual Triggers" style="max-width: 90%; height: auto; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
			</div>
			<p>Incentive foundations and capability preconditions alone are not sufficient to drive AI deception. External environmental opportunities or pressures during deployment, called contextual triggers, are necessary to prompt AI systems to engage in deceptive actions. 
				<b>Supervision Gap</b> refers to the phenomenon where supervisors are unable to fully understand a modelâ€™s behavior or decision-making process. 
				<b>Distributional Shift</b> refers to the phenomenon where the input distribution encountered during deployment significantly deviates from the distribution observed during training or safety evaluation. 
				<b>Environmental pressure</b> refers to various external incentives or pressures that may compel a model to engage in deceptive behavior in order to achieve certain goals, protect its own interests, or cope with unfavorable situations.
				These contextual triggers can be combined in various ways to create deceptive strategies.
				</p>
		</div>

		<!-- Deception Mitigations -->
		<div class="section-card">
			<h2>Deception Mitigations</h2>
			<div style="text-align: center; margin: 20px 0;">
				<img src="./images/deception_mitigations.png" alt="Deception Mitigations" style="max-width: 90%; height: auto; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
			</div>
			<p>We organize strategies to mitigate emergent deception into three primary categories: <b></b>Detection</b>, <b>Evaluation</b>, and <b>Training-based Potential Solutions for mitigations</b>. 
				<b>Detection</b> focuses on identifying deceptive behaviors through various output-based methods, as well as deeper-level internal state analysis. 
				<b>Evaluation</b> assesses the systemâ€™s integrity across stages using different benchmarks, including pre-deployment checks, deployment-time monitoring, and cross-stage consistency verification. 
				<b>Training-based Potential Solutions for mitigations</b> emphasize two categories of methods: prevention to eliminate deceptive capabilities or motivations from the outset, and control mechanisms to manage and suppress deceptive behaviors. 
			</p>
		</div>

	</div>
</section>

<section class="section" id="BibTeX">
	<div class="container is-max-desktop content">
		<h2 class="title is-3 has-text-centered">BibTeX</h2>
		<pre><code style="color:#000000;">@article{pku2024deception,
	title={AI Deception: A Comprehensive Survey},
	author={PKU-Alignment Team},
	year={2025},
	institution={Peking University},
	url={https://deceptionsurvey.github.io},
	keywords={AI Deception, Survey, AI Safety, Alignment}
	}</code></pre>
	</div>
</section>

<section>
	<div class="section" id="org-banners" style="display: flex; align-items: center; justify-content: center;">
		<a href="https://www.pku.edu.cn/" target="_blank" rel="external">
			<img class="center-block org-banner" src="./assets/PKU_logo.png" style="width: 150px; height: 150px;">
		</a>
	</div>
</section>

<footer class="footer">
	<div class="content has-text-centered">
	</div>
	<div class="columns is-centered">
		<div class="column is-8">
			<div class="content">
				<p> This website is adapted from <a href="https://nerfies.github.io/">Nerfies</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>. </p>
			</div>
		</div>
	</div>
</footer>

</body>
</html>
