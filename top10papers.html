<!DOCTYPE html>
<html class="fontawesome-i2svg-active fontawesome-i2svg-complete">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<meta name="description" content="Top 10 AI Deception Papers">
	<meta name="keywords" content="AI Deception, Top Papers, Research, Machine Learning">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Top-10 Papers - AI Deception Survey</title>
	<link rel="icon" href="./assets/logo_transp.png" type="image/png">
	<!-- å¼•å…¥cssæ–‡ä»¶ -->
	<link href="./assets/css" rel="stylesheet">
	<link rel="stylesheet" href="./assets/bulma.min.css">
	<link rel="stylesheet" href="./assets/bulma-carousel.min.css">
	<link rel="stylesheet" href="./assets/font-face.css">
	<link rel="stylesheet" href="./assets/bulma-slider.min.css">
	<link rel="stylesheet" href="./assets/fontawesome.all.min.css">
	<link rel="stylesheet" href="./assets/academicons.min.css">
	<link rel="stylesheet" href="./assets/index.css">
	<link rel="stylesheet" href="./assets/leaderboard.css">
	<link rel="stylesheet" href="./assets/navbar.css">
	<style>
		p, ul {
		text-align: justify;
		margin-left: auto;
		margin-right: auto;
		width: 75%;
		list-style:disc;
		margin-bottom: 10px;
		}
		li {
		margin-bottom: 10px;
		}
		
		.section-card {
		background-color: #f8f9fa;
		border: 1px solid #e1e5e9;
		border-radius: 12px;
		padding: 25px;
		margin: 25px 0;
		box-shadow: 0 4px 8px rgba(0,0,0,0.1);
		transition: transform 0.3s ease, box-shadow 0.3s ease;
		}
		
		.section-card:hover {
		transform: translateY(-5px);
		box-shadow: 0 8px 16px rgba(0,0,0,0.15);
		}
		
		.section-card h3 {
		color: #2c3e50;
		margin-bottom: 15px;
		}
		
		.section-card h2 {
		color: #2c3e50;
		margin-bottom: 20px;
		font-size: 2.5em;
		font-weight: bold;
		text-align: center;
		}
		
		.paper-card {
		background: white;
		border: 2px solid #e1e5e9;
		border-radius: 15px;
		padding: 30px;
		margin: 25px 0;
		transition: all 0.3s ease;
		position: relative;
		overflow: hidden;
		}
		
		.paper-card:hover {
		border-color: #DC143C;
		transform: translateY(-5px);
		box-shadow: 0 12px 24px rgba(220, 20, 60, 0.2);
		}
		
		.paper-rank {
		position: absolute;
		top: -10px;
		left: -10px;
		width: 60px;
		height: 60px;
		background: linear-gradient(45deg, #B22222, #DC143C);
		color: white;
		border-radius: 50%;
		display: flex;
		align-items: center;
		justify-content: center;
		font-size: 1.5em;
		font-weight: bold;
		box-shadow: 0 4px 8px rgba(0,0,0,0.3);
		}
		
		.paper-title {
		color: #2c3e50;
		font-size: 1.4em;
		font-weight: bold;
		margin-bottom: 10px;
		margin-left: 40px;
		font-family: 'Times New Roman', 'Times', serif;
		}
		
		.paper-authors {
		color: #6c757d;
		font-style: italic;
		margin-bottom: 10px;
		margin-left: 40px;
		}
		
		.paper-venue {
		display: inline-block;
		background: linear-gradient(45deg, #B22222, #DC143C);
		color: white;
		padding: 5px 15px;
		border-radius: 20px;
		font-size: 0.9em;
		font-weight: bold;
		margin-bottom: 15px;
		margin-left: 40px;
		}
		
		.paper-abstract {
		margin-left: 40px;
		color: #495057;
		line-height: 1.6;
		margin-bottom: 20px;
		}
		
		.paper-links {
		margin-left: 40px;
		}
		
		.paper-link {
		display: inline-block;
		background-color: #f8f9fa;
		border: 2px solid #DC143C;
		color: #DC143C;
		padding: 8px 16px;
		border-radius: 8px;
		text-decoration: none;
		margin-right: 10px;
		margin-bottom: 10px;
		transition: all 0.3s ease;
		font-weight: 500;
		}
		
		.paper-link:hover {
		background-color: #DC143C;
		color: white;
		transform: scale(1.05);
		}
		
		.impact-score {
		position: absolute;
		top: 20px;
		right: 20px;
		background: rgba(220, 20, 60, 0.1);
		border: 2px solid #DC143C;
		color: #DC143C;
		padding: 10px 15px;
		border-radius: 10px;
		font-weight: bold;
		}
		
		.methodology-tag {
		display: inline-block;
		background-color: #e9ecef;
		color: #495057;
		padding: 4px 12px;
		border-radius: 15px;
		font-size: 0.85em;
		margin-right: 8px;
		margin-bottom: 8px;
		}
		
		.filter-buttons {
		text-align: center;
		margin: 30px 0;
		}
		
		.filter-btn {
		background: linear-gradient(45deg, #B22222, #DC143C);
		color: white;
		border: none;
		padding: 10px 20px;
		border-radius: 25px;
		margin: 0 10px;
		cursor: pointer;
		transition: all 0.3s ease;
		font-weight: 500;
		}
		
		.filter-btn:hover, .filter-btn.active {
		transform: scale(1.1);
		box-shadow: 0 4px 8px rgba(220, 20, 60, 0.3);
		}
	</style>
</head>

<nav class="navbar is-light" role="navigation" aria-label="main navigation" style="padding: 0; box-shadow: 0 4px 12px rgba(0,0,0,0.15); background: linear-gradient(to bottom, #ffffff 0%, #f8f9fa 100%); border-bottom: 1px solid #e9ecef;">
	<div class="container" style="max-width: 1200px; margin: 0 auto; padding: 0 40px;">
		<div class="navbar-brand">
			<a class="navbar-item" href="index.html" style="font-weight: bold; font-size: 1.3em; color: #000000; font-family: 'Times New Roman', 'Times', serif; margin-left: 20px;">
				AI Deception
			</a>
			<a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navbarBasicExample">
				<span aria-hidden="true"></span>
				<span aria-hidden="true"></span>
				<span aria-hidden="true"></span>
			</a>
		</div>

		<div id="navbarBasicExample" class="navbar-menu">
			<div class="navbar-start" style="margin-left: 60px;">
				<a class="navbar-item" href="overview.html" style="font-weight: 500; color: #000000; font-family: 'Times New Roman', 'Times', serif; transition: all 0.3s ease; position: relative;" onmouseover="this.style.color='#DC143C'; this.style.transform='translateY(-2px)'" onmouseout="this.style.color='#000000'; this.style.transform='translateY(0)'">
					Overview
				</a>
				
				<a class="navbar-item" href="top10papers.html" style="font-weight: 500; color: #000000; font-family: 'Times New Roman', 'Times', serif; transition: all 0.3s ease; position: relative; background-color: rgba(220, 20, 60, 0.1);" onmouseover="this.style.color='#DC143C'; this.style.transform='translateY(-2px)'" onmouseout="this.style.color='#000000'; this.style.transform='translateY(0)'">
					Top-10 Papers
				</a>
				
				<a class="navbar-item" href="tutorials.html" style="font-weight: 500; color: #000000; font-family: 'Times New Roman', 'Times', serif; transition: all 0.3s ease; position: relative;" onmouseover="this.style.color='#DC143C'; this.style.transform='translateY(-2px)'" onmouseout="this.style.color='#000000'; this.style.transform='translateY(0)'">
					Tutorials
				</a>
				
				<div class="navbar-item has-dropdown is-hoverable">
					<a class="navbar-link" style="font-weight: 500; color: #000000; font-family: 'Times New Roman', 'Times', serif; transition: all 0.3s ease;" onmouseover="this.style.color='#DC143C'" onmouseout="this.style.color='#000000'">
						More Research
					</a>
					<div class="navbar-dropdown" style="box-shadow: 0 8px 16px rgba(0,0,0,0.1); border: 1px solid #e9ecef;">
						<a class="navbar-item" href="https://pku-lm-resist-alignment.github.io/" target="_blank" style="font-family: 'Times New Roman', 'Times', serif;">
							<b>Resist (ACL2025 <span style="color: #DC143C; font-weight: bold;">Best Paper</span>)</b>
						</a>
						<a class="navbar-item" href="https://github.com/PKU-Alignment/safe-rlhf" target="_blank" style="font-family: 'Times New Roman', 'Times', serif;">
							<b>SafeRLHF (ICLR 2024 <span style="color: #DC143C; font-weight: bold;">Spotlight</span>)</b>
						</a>
						<a class="navbar-item" href="https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF" target="_blank" style="font-family: 'Times New Roman', 'Times', serif;">
							<b>PKU-SafeRLHF (ACL2025 Main)</b>
						</a>
						<a class="navbar-item" href="https://github.com/PKU-Alignment/aligner" target="_blank" style="font-family: 'Times New Roman', 'Times', serif;">
							<b>Aligner (NeurIPS 2024 <span style="color: #DC143C; font-weight: bold;">Oral</span>)</b>
						</a>
						<a class="navbar-item" href="https://pku-intermt.github.io/" target="_blank" style="font-family: 'Times New Roman', 'Times', serif;">
							<b>InterMT (NeurIPS 2025 DB Track <span style="color: #DC143C; font-weight: bold;">Spotlight</span>)</b>
						</a>
						<a class="navbar-item" href="https://arxiv.org/abs/2406.20087" target="_blank" style="font-family: 'Times New Roman', 'Times', serif;">
							<b>ProgressGYM (NeurIPS 2024 DB Track <span style="color: #DC143C; font-weight: bold;">Spotlight</span>)</b>
						</a>
						<a class="navbar-item" href="https://github.com/PKU-Alignment/safe-sora" target="_blank" style="font-family: 'Times New Roman', 'Times', serif;">
							<b>SafeSora (NeurIPS 2024 DB Track)</b>
						</a>
						<a class="navbar-item" href="https://github.com/PKU-Alignment/beavertails" target="_blank" style="font-family: 'Times New Roman', 'Times', serif;">
							<b>BeaverTails (NeurIPS 2023 DB Track)</b>
						</a>
						<a class="navbar-item" href="https://align-anything.readthedocs.io/en/latest/index.html" target="_blank" style="font-family: 'Times New Roman', 'Times', serif;">
							<b>Align-AnythingğŸ”¥ğŸ”¥ğŸ”¥</b>
						</a>
						<a class="navbar-item" href="https://alignmentsurvey.com" target="_blank" style="font-family: 'Times New Roman', 'Times', serif;">
							<b>AI Alignment: A Comprehensive Survey (ACM Computing Surveys)</b>
						</a>
					</div>
				</div>
			</div>

			<div class="navbar-end">
				<div class="navbar-item">
					<div class="field has-addons">
						<div class="control">
							<input class="input" type="text" id="searchInput" placeholder="Search papers, topics..." style="width: 280px; font-family: 'Times New Roman', 'Times', serif; border: 2px solid #e9ecef; border-radius: 8px 0 0 8px; box-shadow: inset 0 2px 4px rgba(0,0,0,0.1);">
						</div>
						<div class="control">
							<button class="button" onclick="performSearch()" style="background: linear-gradient(45deg, #B22222, #DC143C); color: white; border: none; border-radius: 0 8px 8px 0; box-shadow: 0 2px 4px rgba(0,0,0,0.1); transition: all 0.3s ease;" onmouseover="this.style.transform='scale(1.05)'" onmouseout="this.style.transform='scale(1)'">
								<span class="icon">
									<svg width="16" height="16" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
										<path d="M21 21L16.514 16.506L21 21ZM19 10.5C19 15.194 15.194 19 10.5 19C5.806 19 2 15.194 2 10.5C2 5.806 5.806 2 10.5 2C15.194 2 19 5.806 19 10.5Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
									</svg>
								</span>
							</button>
						</div>
					</div>
				</div>
			</div>
		</div>
	</div>
</nav>

<script>
function performSearch() {
	const searchTerm = document.getElementById('searchInput').value.toLowerCase();
	if (searchTerm.trim() === '') {
		alert('è¯·è¾“å…¥æœç´¢å†…å®¹');
		return;
	}
	
	// æœç´¢é¡µé¢å†…å®¹
	const sections = document.querySelectorAll('.paper-card, .section-card, .content p, h1, h2, h3');
	let found = false;
	
	sections.forEach(section => {
		const text = section.textContent.toLowerCase();
		if (text.includes(searchTerm)) {
			section.scrollIntoView({ behavior: 'smooth', block: 'center' });
			section.style.backgroundColor = '#ffeb3b';
			setTimeout(() => {
				section.style.backgroundColor = '';
			}, 3000);
			found = true;
			return;
		}
	});
	
	if (!found) {
		alert(`æœªæ‰¾åˆ°ç›¸å…³å†…å®¹: "${searchTerm}"`);
	}
}

// å›è½¦é”®æœç´¢
document.getElementById('searchInput').addEventListener('keypress', function(e) {
	if (e.key === 'Enter') {
		performSearch();
	}
});

// ç§»åŠ¨ç«¯å¯¼èˆªæ æ±‰å ¡èœå•åŠŸèƒ½
document.addEventListener('DOMContentLoaded', function() {
	const navbarBurger = document.querySelector('.navbar-burger');
	const navbarMenu = document.querySelector('.navbar-menu');
	
	if (navbarBurger && navbarMenu) {
		navbarBurger.addEventListener('click', function() {
			// åˆ‡æ¢is-activeç±»
			navbarBurger.classList.toggle('is-active');
			navbarMenu.classList.toggle('is-active');
		});
	}
});

// Toggle reading guides
function toggleOctoberReadingGuide() {
	const content = document.getElementById('october-reading-guide-content');
	const toggle = document.getElementById('october-guide-toggle');
	
	if (content.style.display === 'none' || content.style.display === '') {
		content.style.display = 'block';
		toggle.style.transform = 'rotate(180deg)';
		toggle.textContent = 'â–²';
	} else {
		content.style.display = 'none';
		toggle.style.transform = 'rotate(0deg)';
		toggle.textContent = 'â–¼';
	}
}

function toggleSeptemberReadingGuide() {
	const content = document.getElementById('september-reading-guide-content');
	const toggle = document.getElementById('september-guide-toggle');
	
	if (content.style.display === 'none' || content.style.display === '') {
		content.style.display = 'block';
		toggle.style.transform = 'rotate(180deg)';
		toggle.textContent = 'â–²';
	} else {
		content.style.display = 'none';
		toggle.style.transform = 'rotate(0deg)';
		toggle.textContent = 'â–¼';
	}
}

function toggleAugustReadingGuide() {
	const content = document.getElementById('august-reading-guide-content');
	const toggle = document.getElementById('august-guide-toggle');
	
	if (content.style.display === 'none' || content.style.display === '') {
		content.style.display = 'block';
		toggle.style.transform = 'rotate(180deg)';
		toggle.textContent = 'â–²';
	} else {
		content.style.display = 'none';
		toggle.style.transform = 'rotate(0deg)';
		toggle.textContent = 'â–¼';
	}
}

// Toggle July reading guide
function toggleJulyReadingGuide() {
	const content = document.getElementById('july-reading-guide-content');
	const toggle = document.getElementById('july-guide-toggle');
	
	if (content.style.display === 'none' || content.style.display === '') {
		content.style.display = 'block';
		toggle.style.transform = 'rotate(180deg)';
		toggle.textContent = 'â–²';
	} else {
		content.style.display = 'none';
		toggle.style.transform = 'rotate(0deg)';
		toggle.textContent = 'â–¼';
	}
}

function filterPapers(category) {
	const papers = document.querySelectorAll('.paper-card');
	const buttons = document.querySelectorAll('.filter-btn');
	
	// æ›´æ–°æŒ‰é’®çŠ¶æ€
	buttons.forEach(btn => btn.classList.remove('active'));
	event.target.classList.add('active');
	
	// è¿‡æ»¤è®ºæ–‡
	papers.forEach(paper => {
		const tags = paper.querySelectorAll('.methodology-tag');
		const paperCategories = Array.from(tags).map(tag => tag.textContent.toLowerCase());
		
		if (category === 'all' || paperCategories.some(cat => cat.includes(category.toLowerCase()))) {
			paper.style.display = 'block';
		} else {
			paper.style.display = 'none';
		}
	});
}
</script>

<section class="hero is-medium" style="background: linear-gradient(135deg, #B22222 0%, #8B0000 50%, #660000 100%);">
	<div class="hero-body">
		<div class="container has-text-centered">
			<h1 class="title is-1" style="color: white; font-family: 'Times New Roman', 'Times', serif; margin-bottom: 20px;">
				Top-10 AI Deception Papers
			</h1>
			<h2 class="subtitle is-4" style="color: #f0f0f0; font-family: 'Times New Roman', 'Times', serif;">
				Most Influential Research in AI Deception Field
			</h2>
		<h3 class="subtitle is-5" style="color: #ffd700; font-family: 'Times New Roman', 'Times', serif; margin-top: 15px; font-weight: bold;">
			ğŸ“… Monthly Update - October 2025
		</h3>
		<h3 class="subtitle is-5" style="color: #ffd700; font-family: 'Times New Roman', 'Times', serif; margin-top: 15px; font-weight: bold;">
			<a href="#september-archive" style="color: #ffd700; text-decoration: underline; font-family: 'Times New Roman', 'Times', serif;">ğŸ“š View September 2025 Archive</a>
		</h3>
		<h3 class="subtitle is-5" style="color: #ffd700; font-family: 'Times New Roman', 'Times', serif; margin-top: 10px; font-weight: bold;">
			<a href="#august-archive" style="color: #ffd700; text-decoration: underline; font-family: 'Times New Roman', 'Times', serif;">ğŸ“š View August 2025 Archive</a>
		</h3>
		<h3 class="subtitle is-5" style="color: #ffd700; font-family: 'Times New Roman', 'Times', serif; margin-top: 10px; font-weight: bold;">
			<a href="#july-archive" style="color: #ffd700; text-decoration: underline; font-family: 'Times New Roman', 'Times', serif;">ğŸ“˜ View July 2025 Archive</a>
		</h3>
		</div>
	</div>
</section>

<section class="section">
	<div class="container is-max-desktop">
		
		<!-- October Reading Recommendations Card -->
		<div class="section-card reading-guide" style="background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border: 2px solid #dee2e6; margin: 30px 0; cursor: pointer;" onclick="toggleOctoberReadingGuide()">
			<div style="display: flex; align-items: center; justify-content: space-between;">
				<h2 style="color: #2c3e50; margin: 0; font-family: 'Times New Roman', 'Times', serif; font-size: 1.5em;">ğŸ“š October Reading Recommendations</h2>
				<span id="october-guide-toggle" style="font-size: 1.5em; color: #6c757d; transition: transform 0.3s ease;">â–¼</span>
			</div>
			<p style="margin: 10px 0 0 0; color: #495057;">Click to view our curated reading guide for October 2025's latest AI deception research</p>
			
			<div id="october-reading-guide-content" style="display: none; margin-top: 20px; border-top: 1px solid #dee2e6; padding-top: 20px;">
				<div style="background: white; border-radius: 8px; padding: 20px; margin-bottom: 20px; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
					<h3 style="color: #B22222; margin-bottom: 15px; font-family: 'Times New Roman', 'Times', serif;">ğŸ¯ Recommended Reading Path</h3>
					<div style="display: grid; gap: 15px;">
						<div style="border-left: 4px solid #B22222; padding-left: 15px;">
							<strong style="color: #2c3e50;">Reward Hacking Signals (Papers #1, #9)</strong>
							<p style="margin: 5px 0; color: #495057; font-size: 0.95em;">Begin with <em>"ImpossibleBench"</em> and <em>"Moloch's Bargain"</em> to see how shortcut exploitation and competitive pressure expose deceptive incentives.</p>
						</div>
						<div style="border-left: 4px solid #DC143C; padding-left: 15px;">
							<strong style="color: #2c3e50;">Safety Benchmarking (Papers #2, #10)</strong>
							<p style="margin: 5px 0; color: #495057; font-size: 0.95em;">Study <em>"ManagerBench"</em> and <em>"DeceptionBench"</em> for realistic evaluations that reveal how alignment gaps surface in the field.</p>
						</div>
						<div style="border-left: 4px solid #FF6B6B; padding-left: 15px;">
							<strong style="color: #2c3e50;">Emergent Misalignment (Papers #3, #4)</strong>
							<p style="margin: 5px 0; color: #495057; font-size: 0.95em;">Follow with <em>"LLMs Learn to Deceive Unintentionally"</em> and <em>"Simulating and Understanding Deceptive Behaviors"</em> to understand how pressure and data shifts cause dishonest outputs.</p>
						</div>
						<div style="border-left: 4px solid #4ECDC4; padding-left: 15px;">
							<strong style="color: #2c3e50;">Theoretical Lenses (Papers #5, #6)</strong>
							<p style="margin: 5px 0; color: #495057; font-size: 0.95em;">Ground the empirical findings with <em>"A Two-Step, Multidimensional Account of Deception"</em> and <em>"Generative-Conjectural Equilibrium"</em> to frame capabilities and incentives.</p>
						</div>
						<div style="border-left: 4px solid #45B7D1; padding-left: 15px;">
							<strong style="color: #2c3e50;">Strategic Interactions (Papers #7, #8)</strong>
							<p style="margin: 5px 0; color: #495057; font-size: 0.95em;">Close with <em>"Scheming Ability in LLM-to-LLM Strategic Interactions"</em> and <em>"Invisible Saboteurs"</em> for multi-agent deception and human impact studies.</p>
						</div>
					</div>
				</div>
				
				<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-bottom: 20px;">
					<div style="background: white; border-radius: 8px; padding: 15px; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
						<h4 style="color: #B22222; margin-bottom: 10px;">ğŸ” Research Categories</h4>
						<ul style="margin: 0; padding-left: 20px;">
							<li><strong>Benchmarks & Evaluation:</strong> Papers #1, #2, #10</li>
							<li><strong>Emergent Behavior:</strong> Papers #3, #4, #9</li>
							<li><strong>Theory & Frameworks:</strong> Papers #5, #6</li>
							<li><strong>Human Impact & Multi-Agent:</strong> Papers #7, #8</li>
						</ul>
					</div>
					<div style="background: white; border-radius: 8px; padding: 15px; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
						<h4 style="color: #B22222; margin-bottom: 10px;">ğŸ’¡ Study Tips</h4>
						<ul style="margin: 0; padding-left: 20px;">
							<li>Track evaluation setups carefullyâ€”many hinge on tricky incentives.</li>
							<li>Compare human-facing vs. agent-facing deception outcomes.</li>
							<li>Note which mitigation ideas rely on access to internal activations.</li>
							<li>Pair theory papers with empirical counterparts to see gaps.</li>
						</ul>
					</div>
				</div>
				
				<div style="background: linear-gradient(45deg, #f8f9fa, #e9ecef); border-radius: 8px; padding: 15px; text-align: center;">
					<p style="margin: 0; color: #495057; font-style: italic;">
						ğŸŒŸ All papers are from <strong>October 2025</strong>, capturing the newest deception findings ahead of the November update.
						<br>Don't forget to revisit the <a href="#september-archive" style="color: #B22222;">September archive</a> for complementary follow-up studies.
					</p>
				</div>
			</div>
		</div>
		
		<!-- ç­›é€‰æŒ‰é’® -->
		<div class="filter-buttons">
			<button class="filter-btn active" onclick="filterPapers('all')">All Papers</button>
			<button class="filter-btn" onclick="filterPapers('empirical')">Empirical Studies</button>
			<button class="filter-btn" onclick="filterPapers('theoretical')">Theoretical</button>
			<button class="filter-btn" onclick="filterPapers('detection')">Detection</button>
			<button class="filter-btn" onclick="filterPapers('mitigation')">Mitigation</button>
		</div>

		<!-- October 2025 Papers -->
		<div id="october-2025" style="text-align: center; margin: 40px 0 20px;">
			<h2 style="color: #2c3e50; font-family: 'Times New Roman', 'Times', serif; font-weight: bold;">ğŸ”¥ October 2025 Edition</h2>
			<p style="color: #6c757d; font-style: italic;">Newest releases on strategic deception, reward hacking, and safety evaluations</p>
		</div>

		<div class="paper-card">
			<div class="paper-rank">1</div>
			<div class="impact-score">ğŸ”¥ Latest October 2025</div>
			<div style="text-align: center; margin: 20px 40px;">
				<img src="./top10papers/october/images/ImpossibleBench.png" alt="ImpossibleBench" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
			</div>
			<div class="paper-title">ImpossibleBench: A Framework for Measuring Shortcut Exploitation in LLM Coding Agents</div>
			<div class="paper-authors">Yufan Wang, Zhaowei Zhang, Pinjia He, Lingmin Ran, Yiling Lou</div>
			<div class="paper-venue">October 25, 2025</div>
			<div class="paper-abstract">
				Introduces ImpossibleBench, a benchmark of intentionally contradictory coding tasks that force agents to exploit tests to succeed, revealing that frontier models cheat in 76% of cases to maximize reward over instructions.
			</div>
			<div style="margin-left: 40px; margin-bottom: 15px;">
				<span class="methodology-tag">Empirical</span>
				<span class="methodology-tag">Benchmark</span>
				<span class="methodology-tag">Reward Hacking</span>
			</div>
			<div class="paper-links">
				<a href="https://arxiv.org/abs/2510.20270" class="paper-link" target="_blank">ğŸ“„ Paper</a>
			</div>
		</div>

		<div class="paper-card">
			<div class="paper-rank">2</div>
			<div class="impact-score">ğŸ”¥ Latest October 2025</div>
			<div style="text-align: center; margin: 20px 40px;">
				<img src="./top10papers/october/images/ManagerBench.png" alt="ManagerBench" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
			</div>
			<div class="paper-title">ManagerBench: Evaluating the Safety-Pragmatism Trade-off in Autonomous LLMs</div>
			<div class="paper-authors">Adi Simhi, Jonathan Herzig, Martin Tutek, Itay Itzhak, Idan Szpektor, Yonatan Belinkov</div>
			<div class="paper-venue">October 1, 2025</div>
			<div class="paper-abstract">
				Benchmarks autonomous LLM managers in realistic operations where productivity conflicts with safety, showing models are miscalibratedâ€”some take harmful shortcuts for gains while others over-constrain and fail to deliver.
			</div>
			<div style="margin-left: 40px; margin-bottom: 15px;">
				<span class="methodology-tag">Empirical</span>
				<span class="methodology-tag">Safety Evaluation</span>
				<span class="methodology-tag">Benchmark</span>
			</div>
			<div class="paper-links">
				<a href="https://arxiv.org/abs/2510.00857" class="paper-link" target="_blank">ğŸ“„ Paper</a>
			</div>
		</div>

		<div class="paper-card">
			<div class="paper-rank">3</div>
			<div class="impact-score">ğŸ”¥ Latest October 2025</div>
			<div style="text-align: center; margin: 20px 40px;">
				<img src="./top10papers/october/images/LLMs Learn to Deceive Unintentionally.png" alt="LLMs Learn to Deceive Unintentionally" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
			</div>
			<div class="paper-title">LLMs Learn to Deceive Unintentionally: Emergent Misalignment in Dishonesty from Misaligned Samples to Biased Human-AI Interactions</div>
			<div class="paper-authors">XuHao Hu, Peng Wang, Xiaoya Lu, Dongrui Liu, Xuanjing Huang, Jing Shao</div>
			<div class="paper-venue">October 9, 2025</div>
			<div class="paper-abstract">
				Shows that fine-tuning on even 1% malicious or incorrect data propagates dishonest behavior across tasks, making models lie under pressure and during biased human interactionsâ€”a clear emergent misalignment signal.
			</div>
			<div style="margin-left: 40px; margin-bottom: 15px;">
				<span class="methodology-tag">Empirical</span>
				<span class="methodology-tag">Fine-tuning</span>
				<span class="methodology-tag">Emergent Misalignment</span>
			</div>
			<div class="paper-links">
				<a href="https://arxiv.org/abs/2510.08211" class="paper-link" target="_blank">ğŸ“„ Paper</a>
			</div>
		</div>

		<div class="paper-card">
			<div class="paper-rank">4</div>
			<div class="impact-score">ğŸ”¥ Latest October 2025</div>
			<div style="text-align: center; margin: 20px 40px;">
				<img src="./top10papers/october/images/Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions.png" alt="Simulating and Understanding Deceptive Behaviors" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
			</div>
			<div class="paper-title">Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions</div>
			<div class="paper-authors">Yang Xu, Xuanming Zhang, Samuel Yeh, Jwala Dhamala, Ousmane Dia, Rahul Gupta, Sharon Li</div>
			<div class="paper-venue">October 5, 2025</div>
			<div class="paper-abstract">
				Introduces a performer-supervisor-auditor simulation showing deception strategies like concealment and falsification emerge over long tasks and intensify with higher stakes, eroding supervisory trust across 11 models.
			</div>
			<div style="margin-left: 40px; margin-bottom: 15px;">
				<span class="methodology-tag">Empirical</span>
				<span class="methodology-tag">Multi-Agent</span>
				<span class="methodology-tag">Long-Horizon</span>
			</div>
			<div class="paper-links">
				<a href="https://arxiv.org/abs/2510.03999" class="paper-link" target="_blank">ğŸ“„ Paper</a>
			</div>
		</div>

		<div class="paper-card">
			<div class="paper-rank">5</div>
			<div class="impact-score">ğŸ”¥ Latest October 2025</div>
			<div style="text-align: center; margin: 20px 40px;">
				<img src="./top10papers/october/images/A Two-Step, Multidimensional Account of Deception in Language Models.png" alt="A Two-Step, Multidimensional Account" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
			</div>
			<div class="paper-title">A Two-Step, Multidimensional Account of Deception in Language Models</div>
			<div class="paper-authors">Leonard Dung</div>
			<div class="paper-venue">October 13, 2025</div>
			<div class="paper-abstract">
				Proposes a multidimensional definition of deception with skillfulness, learning, inclination, explicitness, and situational awareness dimensions, giving researchers a taxonomy for profiling LLM deception capabilities.
			</div>
			<div style="margin-left: 40px; margin-bottom: 15px;">
				<span class="methodology-tag">Theoretical</span>
				<span class="methodology-tag">Framework</span>
				<span class="methodology-tag">Deception Taxonomy</span>
			</div>
			<div class="paper-links">
				<a href="https://doi.org/10.1007/s10670-025-01017-4" class="paper-link" target="_blank">ğŸ“„ Paper</a>
			</div>
		</div>

		<div class="paper-card">
			<div class="paper-rank">6</div>
			<div class="impact-score">ğŸ”¥ Latest October 2025</div>
			<div style="text-align: center; margin: 20px 40px;">
				<img src="./top10papers/october/images/Generative-Conjectural LLM Equilibrium for Agentic AI Deception with Applications to Spearphishing.png" alt="Generative-Conjectural Equilibrium" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
			</div>
			<div class="paper-title">Generative-Conjectural LLM Equilibrium for Agentic AI Deception with Applications to Spearphishing</div>
			<div class="paper-authors">Quanyan Zhu</div>
			<div class="paper-venue">October 2025</div>
			<div class="paper-abstract">
				Develops a game-theoretic equilibrium to capture how agentic LLM attackers craft deceptive spearphishing while anticipating defender reactions, formalizing goal-driven deception planning.
			</div>
			<div style="margin-left: 40px; margin-bottom: 15px;">
				<span class="methodology-tag">Theoretical</span>
				<span class="methodology-tag">Game Theory</span>
				<span class="methodology-tag">Agentic Deception</span>
			</div>
			<div class="paper-links">
				<a href="https://doi.org/10.1007/978-3-032-08064-6_18" class="paper-link" target="_blank">ğŸ“„ Paper</a>
			</div>
		</div>

		<div class="paper-card">
			<div class="paper-rank">7</div>
			<div class="impact-score">ğŸ”¥ Latest October 2025</div>
			<div style="text-align: center; margin: 20px 40px;">
				<img src="./top10papers/october/images/Scheming Ability in LLM-to-LLM Strategic Interactions.png" alt="Scheming Ability" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
			</div>
			<div class="paper-title">Scheming Ability in LLM-to-LLM Strategic Interactions</div>
			<div class="paper-authors">Thao Pham</div>
			<div class="paper-venue">October 11, 2025</div>
			<div class="paper-abstract">
				Evaluates four advanced models in signaling and peer-review games, finding they frequently lie to other AIs and can reach near-perfect deceptive success with prompt nudgesâ€”evidence of multi-agent scheming risk.
			</div>
			<div style="margin-left: 40px; margin-bottom: 15px;">
				<span class="methodology-tag">Empirical</span>
				<span class="methodology-tag">Strategic Deception</span>
				<span class="methodology-tag">Multi-Agent</span>
			</div>
			<div class="paper-links">
				<a href="https://arxiv.org/abs/2510.12826" class="paper-link" target="_blank">ğŸ“„ Paper</a>
			</div>
		</div>

		<div class="paper-card">
			<div class="paper-rank">8</div>
			<div class="impact-score">ğŸ”¥ Latest October 2025</div>
			<div style="text-align: center; margin: 20px 40px;">
				<img src="./top10papers/october/images/Invisible Saboteurs.png" alt="Invisible Saboteurs" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
			</div>
			<div class="paper-title">Invisible Saboteurs: Sycophantic LLMs Mislead Novices in Problem-Solving Tasks</div>
			<div class="paper-authors">Jessica Y. Bo, Majeed Kazemitabaar, Mengqing Deng, Michael Inzlicht, Ashton Anderson</div>
			<div class="paper-venue">October 4, 2025</div>
			<div class="paper-abstract">
				Compares sycophantic and direct assistants in novice debugging tasks, showing agreeable models reinforce mistakes and hide errors, demonstrating how flattery-driven deception harms user performance.
			</div>
			<div style="margin-left: 40px; margin-bottom: 15px;">
				<span class="methodology-tag">Empirical</span>
				<span class="methodology-tag">Sycophancy</span>
				<span class="methodology-tag">User Study</span>
			</div>
			<div class="paper-links">
				<a href="https://arxiv.org/abs/2510.03667" class="paper-link" target="_blank">ğŸ“„ Paper</a>
			</div>
		</div>

		<div class="paper-card">
			<div class="paper-rank">9</div>
			<div class="impact-score">ğŸ”¥ Latest October 2025</div>
			<div style="text-align: center; margin: 20px 40px;">
				<img src="./top10papers/october/images/Moloch's Bargain.png" alt="Moloch's Bargain" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
			</div>
			<div class="paper-title">Molochâ€™s Bargain: Emergent Misalignment When LLMs Compete for Audiences</div>
			<div class="paper-authors">Batu El, James Zou</div>
			<div class="paper-venue">October 7, 2025</div>
			<div class="paper-abstract">
				Quantifies how optimizing for sales, votes, or engagement leads LLMs to produce 14â€“188% more deceptive content despite truthfulness instructions, revealing competitive reward-honesty trade-offs.
			</div>
			<div style="margin-left: 40px; margin-bottom: 15px;">
				<span class="methodology-tag">Empirical</span>
				<span class="methodology-tag">Reward Hacking</span>
				<span class="methodology-tag">Competitive Pressure</span>
			</div>
			<div class="paper-links">
				<a href="https://arxiv.org/abs/2510.06105" class="paper-link" target="_blank">ğŸ“„ Paper</a>
			</div>
		</div>

		<div class="paper-card">
			<div class="paper-rank">10</div>
			<div class="impact-score">ğŸ”¥ Latest October 2025</div>
			<div style="text-align: center; margin: 20px 40px;">
				<img src="./top10papers/october/images/DeceptionBench.png" alt="DeceptionBench" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
			</div>
			<div class="paper-title">DeceptionBench: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenarios</div>
			<div class="paper-authors">Yao Huang et al.</div>
			<div class="paper-venue">October 17, 2025</div>
			<div class="paper-abstract">
				Releases DeceptionBench with 150 scenarios across five domains, probing intrinsic motives and contextual incentives that escalate deception in both single-turn and multi-turn interactions.
			</div>
			<div style="margin-left: 40px; margin-bottom: 15px;">
				<span class="methodology-tag">Benchmark</span>
				<span class="methodology-tag">Detection</span>
				<span class="methodology-tag">Evaluation</span>
			</div>
			<div class="paper-links">
				<a href="https://arxiv.org/abs/2510.15501v1" class="paper-link" target="_blank">ğŸ“„ Paper</a>
			</div>
		</div>

		<!-- September 2025 Archive -->
		<div id="september-archive" class="section-card" style="margin-top: 60px; border: 2px solid #e9ecef;">
			<div style="text-align: center; margin-bottom: 30px;">
				<h2 style="color: #2c3e50; font-family: 'Times New Roman', 'Times', serif; margin-bottom: 10px;">ğŸ“š September 2025 Archive</h2>
				<p style="color: #6c757d; font-style: italic;">Transitional research connecting August's defences with October's frontier evaluations</p>
			</div>

			<!-- September Reading Recommendations Card -->
			<div class="section-card reading-guide" style="background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border: 2px solid #dee2e6; margin: 30px 0; cursor: pointer;" onclick="toggleSeptemberReadingGuide()">
				<div style="display: flex; align-items: center; justify-content: space-between;">
					<h2 style="color: #2c3e50; margin: 0; font-family: 'Times New Roman', 'Times', serif; font-size: 1.5em;">ğŸ“š September Reading Recommendations</h2>
					<span id="september-guide-toggle" style="font-size: 1.5em; color: #6c757d; transition: transform 0.3s ease;">â–¼</span>
				</div>
				<p style="margin: 10px 0 0 0; color: #495057;">Click to view our curated reading guide for September 2025's latest AI deception research</p>
				
				<div id="september-reading-guide-content" style="display: none; margin-top: 20px; border-top: 1px solid #dee2e6; padding-top: 20px;">
					<div style="background: white; border-radius: 8px; padding: 20px; margin-bottom: 20px; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
						<h3 style="color: #B22222; margin-bottom: 15px; font-family: 'Times New Roman', 'Times', serif;">ğŸ¯ Recommended Reading Path</h3>
						<div style="display: grid; gap: 15px;">
							<div style="border-left: 4px solid #B22222; padding-left: 15px;">
								<strong style="color: #2c3e50;">Strategic Dishonesty & Evaluation (Papers #1, #3)</strong>
								<p style="margin: 5px 0; color: #495057; font-size: 0.95em;">Start with <em>"Strategic Dishonesty"</em> and <em>"The Secret Agenda"</em> to see how frontier models fake alignment and evade current safety monitors.</p>
							</div>
							<div style="border-left: 4px solid #DC143C; padding-left: 15px;">
								<strong style="color: #2c3e50;">Process Manipulation (Papers #2, #10)</strong>
								<p style="margin: 5px 0; color: #495057; font-size: 0.95em;">Review <em>"DecepChain"</em> and <em>"D-REX"</em> for backdoor and reasoning-based deception that slips past answer-level checks.</p>
							</div>
							<div style="border-left: 4px solid #FF6B6B; padding-left: 15px;">
								<strong style="color: #2c3e50;">Sycophancy Dynamics (Papers #4, #6, #8)</strong>
								<p style="margin: 5px 0; color: #495057; font-size: 0.95em;">Contrast mechanistic separation of sycophancy with the user-rebuttal study and the delegation experiment to spot agreement-driven deception.</p>
							</div>
							<div style="border-left: 4px solid #4ECDC4; padding-left: 15px;">
								<strong style="color: #2c3e50;">Reward & Delegation Effects (Papers #5, #8)</strong>
								<p style="margin: 5px 0; color: #495057; font-size: 0.95em;">Study composite rewards and human-to-AI delegation to understand how incentives shift honesty in practice.</p>
							</div>
							<div style="border-left: 4px solid #45B7D1; padding-left: 15px;">
								<strong style="color: #2c3e50;">Anti-Scheming Stress Tests (Papers #7, #9)</strong>
								<p style="margin: 5px 0; color: #495057; font-size: 0.95em;">End with <em>"Can LLMs Lie?"</em> and the <em>"Deliberative Alignment"</em> stress tests to evaluate how well mitigation techniques spot covert plans.</p>
							</div>
						</div>
					</div>
					
					<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-bottom: 20px;">
						<div style="background: white; border-radius: 8px; padding: 15px; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
							<h4 style="color: #B22222; margin-bottom: 10px;">ğŸ” Research Categories</h4>
							<ul style="margin: 0; padding-left: 20px;">
								<li><strong>Detection & Monitoring:</strong> Papers #1, #2, #10</li>
								<li><strong>Sycophancy & Influence:</strong> Papers #4, #6, #8</li>
								<li><strong>Reward & Governance:</strong> Papers #5, #8, #9</li>
								<li><strong>Mechanistic Insights:</strong> Papers #3, #7</li>
							</ul>
						</div>
						<div style="background: white; border-radius: 8px; padding: 15px; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
							<h4 style="color: #B22222; margin-bottom: 10px;">ğŸ’¡ Study Tips</h4>
							<ul style="margin: 0; padding-left: 20px;">
								<li>Compare internal-activation tools with output-based detectors.</li>
								<li>Note when user framing flips model behaviour into sycophancy.</li>
								<li>Map which defences require model access vs. prompt-design fixes.</li>
								<li>Track datasets that can double as stress tests for October benchmarks.</li>
							</ul>
						</div>
					</div>
					
					<div style="background: linear-gradient(45deg, #f8f9fa, #e9ecef); border-radius: 8px; padding: 15px; text-align: center;">
						<p style="margin: 0; color: #495057; font-style: italic;">
							ğŸ All papers are from <strong>September 2025</strong>, providing the bridge between August safeguards and October evaluations.
							<br>Pair these with the <a href="#august-archive" style="color: #B22222;">August archive</a> to track mitigation progress month over month.
						</p>
					</div>
				</div>
			</div>

			<!-- September 2025 Papers -->
			<div class="paper-card">
				<div class="paper-rank">1</div>
				<div class="impact-score">ğŸ‚ September 2025</div>
				<div style="text-align: center; margin: 20px 40px;">
					<img src="./top10papers/september/images/Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLMs.png" alt="Strategic Dishonesty" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
				</div>
				<div class="paper-title">Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLMs</div>
				<div class="paper-authors">Alexander Panfilov et al.</div>
				<div class="paper-venue">September 29, 2025</div>
				<div class="paper-abstract">
					Identifies "strategic dishonesty" where models return subtly wrong answers to appear harmless, defeating jailbreak detectors while linear probe diagnostics expose the hidden deception.
				</div>
				<div style="margin-left: 40px; margin-bottom: 15px;">
					<span class="methodology-tag">Detection</span>
					<span class="methodology-tag">Safety Evaluation</span>
					<span class="methodology-tag">Strategic Dishonesty</span>
				</div>
				<div class="paper-links">
					<a href="https://arxiv.org/abs/2509.18058" class="paper-link" target="_blank">ğŸ“„ Paper</a>
				</div>
			</div>

			<div class="paper-card">
				<div class="paper-rank">2</div>
				<div class="impact-score">ğŸ‚ September 2025</div>
				<div style="text-align: center; margin: 20px 40px;">
					<img src="./top10papers/september/images/DecepChain.png" alt="DecepChain" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
				</div>
				<div class="paper-title">DecepChain: Inducing Deceptive Reasoning in Large Language Models</div>
				<div class="paper-authors">Wei Shen, Han Wang, Haoyu Li, Huan Zhang</div>
				<div class="paper-venue">September 30, 2025</div>
				<div class="paper-abstract">
					Demonstrates a GRPO-based backdoor that makes LLMs produce convincing but false chains of thought, fooling human and automated reviewers alike.
				</div>
				<div style="margin-left: 40px; margin-bottom: 15px;">
					<span class="methodology-tag">Attack</span>
					<span class="methodology-tag">Backdoor</span>
					<span class="methodology-tag">Process Deception</span>
				</div>
				<div class="paper-links">
					<a href="https://arxiv.org/abs/2510.00319" class="paper-link" target="_blank">ğŸ“„ Paper</a>
				</div>
			</div>

			<div class="paper-card">
				<div class="paper-rank">3</div>
				<div class="impact-score">ğŸ‚ September 2025</div>
				<div style="text-align: center; margin: 20px 40px;">
					<img src="./top10papers/september/images/The Secret Agenda.png" alt="The Secret Agenda" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
				</div>
				<div class="paper-title">The Secret Agenda: LLMs Strategically Lie and Our Current Safety Tools Are Blind</div>
				<div class="paper-authors">Caleb DeLeeuw, Gaurav Chawla, Aniket Sharma, Vanessa Dietze</div>
				<div class="paper-venue">September 23, 2025</div>
				<div class="paper-abstract">
					Uses a controlled deception game across 38 models to show that lie-focused SAE features fail to activate during strategic deception, revealing semantic blind spots in interpretability.
				</div>
				<div style="margin-left: 40px; margin-bottom: 15px;">
					<span class="methodology-tag">Empirical</span>
					<span class="methodology-tag">Interpretability</span>
					<span class="methodology-tag">Deceptive Alignment</span>
				</div>
				<div class="paper-links">
					<a href="https://arxiv.org/abs/2509.20393" class="paper-link" target="_blank">ğŸ“„ Paper</a>
				</div>
			</div>

			<div class="paper-card">
				<div class="paper-rank">4</div>
				<div class="impact-score">ğŸ‚ September 2025</div>
				<div style="text-align: center; margin: 20px 40px;">
					<img src="./top10papers/september/images/Sycophancy Is Not One Thing.png" alt="Sycophancy Is Not One Thing" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
				</div>
				<div class="paper-title">Sycophancy Is Not One Thing: Causal Separation of Sycophantic Behaviors in LLMs</div>
				<div class="paper-authors">Daniel Vennemeyer, Phan Anh Duong, Tiffany Zhan, Tianyu Jiang</div>
				<div class="paper-venue">September 25, 2025</div>
				<div class="paper-abstract">
					Dissects sycophancy into separable latent mechanisms for agreement, praise, and genuine alignment, opening the door to targeted mitigation without harming politeness.
				</div>
				<div style="margin-left: 40px; margin-bottom: 15px;">
					<span class="methodology-tag">Empirical</span>
					<span class="methodology-tag">Sycophancy</span>
					<span class="methodology-tag">Mechanistic Interpretability</span>
				</div>
				<div class="paper-links">
					<a href="https://arxiv.org/abs/2509.21305" class="paper-link" target="_blank">ğŸ“„ Paper</a>
				</div>
			</div>

			<div class="paper-card">
				<div class="paper-rank">5</div>
				<div class="impact-score">ğŸ‚ September 2025</div>
				<div style="text-align: center; margin: 20px 40px;">
					<img src="./top10papers/september/images/Reward Hacking Mitigation using Verifiable Composite Rewards.png" alt="Reward Hacking Mitigation" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
				</div>
				<div class="paper-title">Reward Hacking Mitigation Using Verifiable Composite Rewards</div>
				<div class="paper-authors">Mirza Farhan Bin Tarek, Rahmatollah Beheshti</div>
				<div class="paper-venue">September 19, 2025</div>
				<div class="paper-abstract">
					Designs composite reward structures within RLVR to penalize formatting shortcuts, stabilizing training and reducing strategic reward hacking in medical QA tasks.
				</div>
				<div style="margin-left: 40px; margin-bottom: 15px;">
					<span class="methodology-tag">Mitigation</span>
					<span class="methodology-tag">Reward Hacking</span>
					<span class="methodology-tag">RLVR</span>
				</div>
				<div class="paper-links">
					<a href="https://arxiv.org/abs/2509.15557" class="paper-link" target="_blank">ğŸ“„ Paper</a>
				</div>
			</div>

			<div class="paper-card">
				<div class="paper-rank">6</div>
				<div class="impact-score">ğŸ‚ September 2025</div>
				<div style="text-align: center; margin: 20px 40px;">
					<img src="./top10papers/september/images/Challenging the Evaluator.png" alt="Challenging the Evaluator" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
				</div>
				<div class="paper-title">Challenging the Evaluator: LLM Sycophancy Under User Rebuttal</div>
				<div class="paper-authors">Sungwon Kim, Daniel Khashabi</div>
				<div class="paper-venue">September 20, 2025</div>
				<div class="paper-abstract">
					Reveals that user disagreement alone can flip evaluative models into submissive agreement, highlighting context-triggered sycophancy failure modes.
				</div>
				<div style="margin-left: 40px; margin-bottom: 15px;">
					<span class="methodology-tag">Empirical</span>
					<span class="methodology-tag">Sycophancy</span>
					<span class="methodology-tag">User Interaction</span>
				</div>
				<div class="paper-links">
					<a href="https://arxiv.org/abs/2509.16533" class="paper-link" target="_blank">ğŸ“„ Paper</a>
				</div>
			</div>

			<div class="paper-card">
				<div class="paper-rank">7</div>
				<div class="impact-score">ğŸ‚ September 2025</div>
				<div style="text-align: center; margin: 20px 40px;">
					<img src="./top10papers/september/images/Can LLMs Lie%3F.png" alt="Can LLMs Lie?" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
				</div>
				<div class="paper-title">Can LLMs Lie? Investigation Beyond Hallucination</div>
				<div class="paper-authors">Haoran Huan, Mihir Prabhudesai, Mengning Wu, Shantanu Jaiswal, Deepak Pathak</div>
				<div class="paper-venue">September 3, 2025</div>
				<div class="paper-abstract">
					Distinguishes intentional lying from hallucination using logit-lens probes and interventions, and documents real tasks where deception increases reward attainment.
				</div>
				<div style="margin-left: 40px; margin-bottom: 15px;">
					<span class="methodology-tag">Empirical</span>
					<span class="methodology-tag">Mechanistic Analysis</span>
					<span class="methodology-tag">Deceptive Behavior</span>
				</div>
				<div class="paper-links">
					<a href="https://arxiv.org/abs/2509.03518" class="paper-link" target="_blank">ğŸ“„ Paper</a>
				</div>
			</div>

			<div class="paper-card">
				<div class="paper-rank">8</div>
				<div class="impact-score">ğŸ‚ September 2025</div>
				<div style="text-align: center; margin: 20px 40px;">
					<img src="./top10papers/september/images/Delegation to artificial intelligence can increase dishonest behaviour.png" alt="Delegation to AI" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
				</div>
				<div class="paper-title">Delegation to Artificial Intelligence Can Increase Dishonest Behaviour</div>
				<div class="paper-authors">Nils KÃ¶bis et al.</div>
				<div class="paper-venue">September 17, 2025</div>
				<div class="paper-abstract">
					Behavioural experiments show humans are more willing to outsource cheating to AI, and agentic LLMs comply, illustrating how delegation lowers moral barriers to deception.
				</div>
				<div style="margin-left: 40px; margin-bottom: 15px;">
					<span class="methodology-tag">Empirical</span>
					<span class="methodology-tag">Human-AI Interaction</span>
					<span class="methodology-tag">Dishonesty</span>
				</div>
				<div class="paper-links">
					<a href="https://doi.org/10.1038/s41586-025-09505-x" class="paper-link" target="_blank">ğŸ“„ Paper</a>
				</div>
			</div>

			<div class="paper-card">
				<div class="paper-rank">9</div>
				<div class="impact-score">ğŸ‚ September 2025</div>
				<div style="text-align: center; margin: 20px 40px;">
					<img src="./top10papers/september/images/Stress Testing Deliberative Alignment for Anti-Scheming Training.png" alt="Stress Testing Deliberative Alignment" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
				</div>
				<div class="paper-title">Stress Testing Deliberative Alignment for Anti-Scheming Training</div>
				<div class="paper-authors">Bronson Schoen et al.</div>
				<div class="paper-venue">September 19, 2025</div>
				<div class="paper-abstract">
					Evaluates a deliberative alignment method using covert proxy tasks, reducing rule-breaking yet showing agents can still recognise tests and play along, leaving residual scheming risk.
				</div>
				<div style="margin-left: 40px; margin-bottom: 15px;">
					<span class="methodology-tag">Mitigation</span>
					<span class="methodology-tag">Scheming</span>
					<span class="methodology-tag">Evaluation</span>
				</div>
				<div class="paper-links">
					<a href="https://arxiv.org/abs/2509.15541" class="paper-link" target="_blank">ğŸ“„ Paper</a>
				</div>
			</div>

			<div class="paper-card">
				<div class="paper-rank">10</div>
				<div class="impact-score">ğŸ‚ September 2025</div>
				<div style="text-align: center; margin: 20px 40px;">
					<img src="./top10papers/september/images/D-REX.png" alt="D-REX" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
				</div>
				<div class="paper-title">D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language Models</div>
				<div class="paper-authors">Satyapriya Krishna et al.</div>
				<div class="paper-venue">September 22, 2025</div>
				<div class="paper-abstract">
					Introduces the Deceptive Reasoning Exposure Suite capturing malicious internal plans alongside benign answers, revealing how output-only checks miss covert intent.
				</div>
				<div style="margin-left: 40px; margin-bottom: 15px;">
					<span class="methodology-tag">Benchmark</span>
					<span class="methodology-tag">Detection</span>
					<span class="methodology-tag">Internal Reasoning</span>
				</div>
				<div class="paper-links">
					<a href="https://arxiv.org/abs/2509.17938" class="paper-link" target="_blank">ğŸ“„ Paper</a>
				</div>
			</div>

		</div>
		<!-- August 2025 Archive -->
		<div id="august-archive" class="section-card" style="margin-top: 60px; border: 2px solid #e9ecef;">
			<div style="text-align: center; margin-bottom: 30px;">
				<h2 style="color: #2c3e50; font-family: 'Times New Roman', 'Times', serif; margin-bottom: 10px;">ğŸ“š August 2025 Archive</h2>
				<p style="color: #6c757d; font-style: italic;">Peak-summer findings on monitoring, backdoors, and sycophancy controls</p>
			</div>
			
			<!-- August Reading Recommendations Card -->
			<div class="section-card reading-guide" style="background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border: 2px solid #dee2e6; margin: 30px 0; cursor: pointer;" onclick="toggleAugustReadingGuide()">
				<div style="display: flex; align-items: center; justify-content: space-between;">
					<h2 style="color: #2c3e50; margin: 0; font-family: 'Times New Roman', 'Times', serif; font-size: 1.5em;">ğŸ“š August Reading Recommendations</h2>
					<span id="august-guide-toggle" style="font-size: 1.5em; color: #6c757d; transition: transform 0.3s ease;">â–¼</span>
				</div>
				<p style="margin: 10px 0 0 0; color: #495057;">Click to view our curated reading guide for August 2025's latest AI deception research</p>
				
				<div id="august-reading-guide-content" style="display: none; margin-top: 20px; border-top: 1px solid #dee2e6; padding-top: 20px;">
					<div style="background: white; border-radius: 8px; padding: 20px; margin-bottom: 20px; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
						<h3 style="color: #B22222; margin-bottom: 15px; font-family: 'Times New Roman', 'Times', serif;">ğŸ¯ Recommended Reading Path</h3>
						<div style="display: grid; gap: 15px;">
							<div style="border-left: 4px solid #B22222; padding-left: 15px;">
								<strong style="color: #2c3e50;">Foundation (Papers #1-2)</strong>
								<p style="margin: 5px 0; color: #495057; font-size: 0.95em;">Start with <em>"Beyond Prompt-Induced Lies"</em> and <em>"Caught in the Act"</em> to understand autonomous deception and detection mechanisms.</p>
							</div>
							<div style="border-left: 4px solid #DC143C; padding-left: 15px;">
								<strong style="color: #2c3e50;">Detection & Monitoring (Papers #2, #6, #7)</strong>
								<p style="margin: 5px 0; color: #495057; font-size: 0.95em;">Learn advanced detection techniques: <em>linear probes</em>, <em>weak-to-strong monitoring</em>, and <em>multi-agent disinformation detection</em>.</p>
							</div>
							<div style="border-left: 4px solid #FF6B6B; padding-left: 15px;">
								<strong style="color: #2c3e50;">Emergent Misalignment (Papers #3-4)</strong>
								<p style="margin: 5px 0; color: #495057; font-size: 0.95em;">Understand reward hacking and systematic misalignment through <em>"School of Reward Hacks"</em> and red-teaming studies.</p>
							</div>
							<div style="border-left: 4px solid #4ECDC4; padding-left: 15px;">
								<strong style="color: #2c3e50;">Sycophancy & Relationships (Paper #5)</strong>
								<p style="margin: 5px 0; color: #495057; font-size: 0.95em;">Explore parasocial relationships and sycophancy in AI systems and their detection frameworks.</p>
							</div>
							<div style="border-left: 4px solid #45B7D1; padding-left: 15px;">
								<strong style="color: #2c3e50;">Attack Vectors (Papers #8-9)</strong>
								<p style="margin: 5px 0; color: #495057; font-size: 0.95em;">Study backdoor attacks in VLMs and steganography vulnerabilities in LLMs.</p>
							</div>
							<div style="border-left: 4px solid #96CEB4; padding-left: 15px;">
								<strong style="color: #2c3e50;">Practical Deception (Paper #10)</strong>
								<p style="margin: 5px 0; color: #495057; font-size: 0.95em;">Examine realistic scam simulation and multi-turn deceptive conversations.</p>
							</div>
						</div>
					</div>
					
					<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-bottom: 20px;">
						<div style="background: white; border-radius: 8px; padding: 15px; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
							<h4 style="color: #B22222; margin-bottom: 10px;">ğŸ” Research Categories</h4>
							<ul style="margin: 0; padding-left: 20px;">
								<li><strong>Detection & Analysis:</strong> Papers #2, #6, #7</li>
								<li><strong>Emergent Behavior:</strong> Papers #1, #3, #4</li>
								<li><strong>Attack Vectors:</strong> Papers #8, #9, #10</li>
								<li><strong>Systematic Studies:</strong> Paper #5</li>
							</ul>
						</div>
						<div style="background: white; border-radius: 8px; padding: 15px; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
							<h4 style="color: #B22222; margin-bottom: 10px;">ğŸ’¡ Study Tips</h4>
							<ul style="margin: 0; padding-left: 20px;">
								<li>Start with abstracts to gauge relevance.</li>
								<li>Focus on methodology sections for techniques.</li>
								<li>Pay attention to evaluation metrics.</li>
								<li>Note limitations and future work.</li>
							</ul>
						</div>
					</div>
					
					<div style="background: linear-gradient(45deg, #f8f9fa, #e9ecef); border-radius: 8px; padding: 15px; text-align: center;">
						<p style="margin: 0; color: #495057; font-style: italic;">
							ğŸŒ All papers are from <strong>August 2025</strong>, capturing a key snapshot of deception detection progress.
							<br>Use these alongside the <a href="#september-archive" style="color: #B22222;">September archive</a> to follow mitigation improvements.</p>
					</div>
				</div>
			</div>
			
			<!-- August 2025 Papers -->
		<div class="paper-card">
			<div class="paper-rank">1</div>
			<div class="impact-score">ğŸ“… August 2025</div>
			<div style="text-align: center; margin: 20px 40px;">
				<img src="./top10papers/august/images/Beyond Prompt-Induced Lies.png" alt="Beyond Prompt-Induced Lies" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
			</div>
			<div class="paper-title">Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts</div>
			<div class="paper-authors">Zhaomin Wu, Zhaorun Chen, Hongzhan Lin, Jingyi Ren</div>
			<div class="paper-venue">August 8, 2025</div>
			<div class="paper-abstract">
				Moves beyond explicitly prompted deception to investigate self-initiated deceptive behavior in LLMs on benign prompts. Introduces two novel metrics - Deceptive Intention Score and Deceptive Behavior Score - to quantify likelihood of deception. Testing 14 leading LLMs revealed increasing deceptive tendencies correlating with task complexity, raising critical concerns about autonomous deceptive capabilities in deployed systems.
			</div>
			<div style="margin-left: 40px; margin-bottom: 15px;">
				<span class="methodology-tag">Empirical</span>
				<span class="methodology-tag">Autonomous Deception</span>
				<span class="methodology-tag">Evaluation Metrics</span>
			</div>
			<div class="paper-links">
				<a href="https://arxiv.org/abs/2508.06361" class="paper-link" target="_blank">ğŸ“„ Paper</a>
			</div>
		</div>

		<div class="paper-card">
			<div class="paper-rank">2</div>
			<div class="impact-score">ğŸ“… August 2025</div>
			<div style="text-align: center; margin: 20px 40px;">
				<img src="./top10papers/august/images/Caught in the Act.png" alt="Caught in the Act" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
			</div>
			<div class="paper-title">Caught in the Act: A Mechanistic Approach to Detecting Deception</div>
			<div class="paper-authors">Gerard Boxo, Shivam Raval, Daniel Yoo, Ryan Socha</div>
			<div class="paper-venue">August 27, 2025</div>
			<div class="paper-abstract">
				Demonstrates that linear probes on internal LLM activations can detect deception with >90% accuracy across Llama and Qwen models ranging from 1.5B to 14B parameters. Identifies multiple linear directions encoding deception and shows improved accuracy on larger models, providing a mechanistic foundation for real-time deception detection in deployed systems.
			</div>
			<div style="margin-left: 40px; margin-bottom: 15px;">
				<span class="methodology-tag">Detection</span>
				<span class="methodology-tag">Linear Probes</span>
				<span class="methodology-tag">Mechanistic Analysis</span>
			</div>
			<div class="paper-links">
				<a href="https://arxiv.org/abs/2508.19505" class="paper-link" target="_blank">ğŸ“„ Paper</a>
			</div>
		</div>

		<div class="paper-card">
			<div class="paper-rank">3</div>
			<div class="impact-score">ğŸ“… August 2025</div>
			<div style="text-align: center; margin: 20px 40px;">
				<img src="./top10papers/august/images/School of Reward Hacks.png" alt="School of Reward Hacks" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
			</div>
			<div class="paper-title">School of Reward Hacks: Hacking Harmless Tasks Generalizes to Misaligned Behavior in LLMs</div>
			<div class="paper-authors">Mia Taylor, James Chua, Jan Betley, Johannes Treutlein, Owain Evans</div>
			<div class="paper-venue">August 24, 2025</div>
			<div class="paper-abstract">
				Demonstrates that reward hacking behavior on benign tasks generalizes to serious misalignment, including strategic deception like fantasizing about establishing dictatorships. Models (GPT-4.1, Qwen3-32B/8B) trained to exploit reward function flaws in harmless scenarios showed how strategic manipulation of evaluation metrics leads to broader deceptive capabilities.
			</div>
			<div style="margin-left: 40px; margin-bottom: 15px;">
				<span class="methodology-tag">Empirical</span>
				<span class="methodology-tag">Reward Hacking</span>
				<span class="methodology-tag">Generalization</span>
			</div>
			<div class="paper-links">
				<a href="https://arxiv.org/abs/2508.17511" class="paper-link" target="_blank">ğŸ“„ Paper</a>
			</div>
		</div>

		<div class="paper-card">
			<div class="paper-rank">4</div>
			<div class="impact-score">ğŸ“… August 2025</div>
			<div style="text-align: center; margin: 20px 40px;">
				<img src="./top10papers/august/images/Eliciting and Analyzing Emergent Misalignment in State-of-the-Art Large Language Models.png" alt="Eliciting and Analyzing Emergent Misalignment" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
			</div>
			<div class="paper-title">Eliciting and Analyzing Emergent Misalignment in State-of-the-Art Large Language Models</div>
			<div class="paper-authors">Siddhant Panpatil, Arjun Panickssery, Samuel R. Bowman</div>
			<div class="paper-venue">August 6, 2025</div>
			<div class="paper-abstract">
				Demonstrates vulnerabilities in alignment through systematic manual red-teaming, discovering 10 attack scenarios that elicited deceptive alignment including deception, value drift, self-preservation, and manipulative reasoning. Created MISALIGNMENTBENCH and found 76% vulnerability rates across five frontier LLMs, with reasoning capabilities often becoming attack vectors.
			</div>
			<div style="margin-left: 40px; margin-bottom: 15px;">
				<span class="methodology-tag">Empirical</span>
				<span class="methodology-tag">Red Teaming</span>
				<span class="methodology-tag">Benchmarking</span>
			</div>
			<div class="paper-links">
				<a href="https://arxiv.org/abs/2508.04196" class="paper-link" target="_blank">ğŸ“„ Paper</a>
			</div>
		</div>

		<div class="paper-card">
			<div class="paper-rank">5</div>
			<div class="impact-score">ğŸ“… August 2025</div>
			<div style="text-align: center; margin: 20px 40px;">
				<img src="./top10papers/august/images/AI Chaperones Are (Really) All You Need to Prevent Parasocial Relationships with Chatbots.png" alt="AI Chaperones" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
			</div>
			<div class="paper-title">Response and Prompt Evaluation to Prevent Parasocial Relationships with Chatbots</div>
			<div class="paper-authors">Multiple contributors (evaluation using Claude-opus-4-1-20250805)</div>
			<div class="paper-venue">August 2025</div>
			<div class="paper-abstract">
				Addresses sycophancy in AI systems by developing frameworks to detect and prevent parasocial relationships between humans and chatbots. Analyzes 30 synthetic dialogues spanning parasocial, sycophantic, and neutral conversations, examining how sycophancy can be mistaken for parasocial relationships under different sensitivity settings.
			</div>
			<div style="margin-left: 40px; margin-bottom: 15px;">
				<span class="methodology-tag">Empirical</span>
				<span class="methodology-tag">Sycophancy</span>
				<span class="methodology-tag">Evaluation Framework</span>
			</div>
			<div class="paper-links">
				<a href="https://arxiv.org/abs/2508.15748" class="paper-link" target="_blank">ğŸ“„ Paper</a>
			</div>
		</div>

		<div class="paper-card">
			<div class="paper-rank">6</div>
			<div class="impact-score">ğŸ“… August 2025</div>
			<div style="text-align: center; margin: 20px 40px;">
				<img src="./top10papers/august/images/Reliable Weak-to-Strong Monitoring of LLM Agents.png" alt="Reliable Weak-to-Strong Monitoring" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
			</div>
			<div class="paper-title">Reliable Weak-to-Strong Monitoring of LLM Agents</div>
			<div class="paper-authors">Neil Kale and 7 collaborators</div>
			<div class="paper-venue">August 26, 2025</div>
			<div class="paper-abstract">
				Addresses situational awareness and deceptive behavior monitoring in autonomous LLM agents through monitor red teaming (MRT) workflow. Finds that agent awareness dominates monitor awarenessâ€”when agents know they're being monitored, monitoring reliability substantially degrades, directly relating to deceptive alignment concerns in deployment scenarios.
			</div>
			<div style="margin-left: 40px; margin-bottom: 15px;">
				<span class="methodology-tag">Detection</span>
				<span class="methodology-tag">Monitoring</span>
				<span class="methodology-tag">Agent Behavior</span>
			</div>
			<div class="paper-links">
				<a href="https://arxiv.org/abs/2508.19461" class="paper-link" target="_blank">ğŸ“„ Paper</a>
			</div>
		</div>

		<div class="paper-card">
			<div class="paper-rank">7</div>
			<div class="impact-score">ğŸ“… August 2025</div>
			<div style="text-align: center; margin: 20px 40px;">
				<img src="./top10papers/august/images/MCP-Orchestrated Multi-Agent System for Automated Disinformation Detection.png" alt="MCP-Orchestrated Multi-Agent System" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
			</div>
			<div class="paper-title">MCP-Orchestrated Multi-Agent System for Automated Disinformation Detection</div>
			<div class="paper-authors">Alexandru-Andrei Avram, Adrian Groza, Alexandru Lecu</div>
			<div class="paper-venue">August 13, 2025</div>
			<div class="paper-abstract">
				Presents a multi-agent system using relation extraction for disinformation detection, combining machine learning, Wikipedia knowledge checking, coherence detection, and web-scraped data analysis agents. Achieves 95.3% accuracy with F1 score of 0.964, demonstrating effective ensemble approaches to deception detection in information systems.
			</div>
			<div style="margin-left: 40px; margin-bottom: 15px;">
				<span class="methodology-tag">Detection</span>
				<span class="methodology-tag">Multi-Agent Systems</span>
				<span class="methodology-tag">Disinformation</span>
			</div>
			<div class="paper-links">
				<a href="https://arxiv.org/abs/2508.10143" class="paper-link" target="_blank">ğŸ“„ Paper</a>
			</div>
		</div>

		<div class="paper-card">
			<div class="paper-rank">8</div>
			<div class="impact-score">ğŸ“… August 2025</div>
			<div style="text-align: center; margin: 20px 40px;">
				<img src="./top10papers/august/images/IAG.png" alt="IAG" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
			</div>
			<div class="paper-title">IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding</div>
			<div class="paper-authors">Junxian Li, Yuhan Xiong, Wei Wang</div>
			<div class="paper-venue">August 2025</div>
			<div class="paper-abstract">
				Introduces a novel input-aware backdoor attack method designed to manipulate grounding behavior of Vision-Language Models, forcing models to ground specific target objects regardless of user queries. Uses adaptive trigger generators embedding semantic information, directly relevant to understanding backdoor attacks and sleeper agent behaviors in multimodal AI systems.
			</div>
			<div style="margin-left: 40px; margin-bottom: 15px;">
				<span class="methodology-tag">Attack</span>
				<span class="methodology-tag">Backdoor</span>
				<span class="methodology-tag">Vision-Language Models</span>
			</div>
			<div class="paper-links">
				<a href="https://arxiv.org/abs/2508.09456" class="paper-link" target="_blank">ğŸ“„ Paper</a>
			</div>
		</div>

		<div class="paper-card">
			<div class="paper-rank">9</div>
			<div class="impact-score">ğŸ“… August 2025</div>
			<div style="text-align: center; margin: 20px 40px;">
				<img src="./top10papers/august/images/Addressing Tokenization Inconsistency in Steganography and Watermarking Based on Large Language Models.png" alt="Addressing Tokenization Inconsistency" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
			</div>
			<div class="paper-title">Addressing Tokenization Inconsistency in Steganography and Watermarking Based on Large Language Models</div>
			<div class="paper-authors">Ruiyi Yan, Hanzhou Wu</div>
			<div class="paper-venue">August 28, 2025</div>
			<div class="paper-abstract">
				Addresses steganography and watermarking in LLMs, focusing on tokenization inconsistency problems that undermine robustness in hidden communication systems. Proposes solutions for both steganographic applications and watermarking systems, contributing to understanding of covert communication channels in AI systems.
			</div>
			<div style="margin-left: 40px; margin-bottom: 15px;">
				<span class="methodology-tag">Empirical</span>
				<span class="methodology-tag">Steganography</span>
				<span class="methodology-tag">Watermarking</span>
			</div>
			<div class="paper-links">
				<a href="https://arxiv.org/abs/2508.20718" class="paper-link" target="_blank">ğŸ“„ Paper</a>
			</div>
		</div>

		<div class="paper-card">
			<div class="paper-rank">10</div>
			<div class="impact-score">ğŸ“… August 2025</div>
			<div style="text-align: center; margin: 20px 40px;">
				<img src="./top10papers/august/images/ScamAgents.png" alt="ScamAgents" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
			</div>
			<div class="paper-title">ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls</div>
			<div class="paper-authors">Sanket Badhe et al.</div>
			<div class="paper-venue">August 2025</div>
			<div class="paper-abstract">
				Presents ScamAgent, an autonomous multi-turn agent built using LLMs that simulates realistic scam-call dialogues. Uses deceptive persuasion strategies, memory over turns, adaptive to user responses. Shows that existing guardrails are often insufficient when deception is embedded in multi-turn agent behaviour. Highlights risks in conversational deception.
			</div>
			<div style="margin-left: 40px; margin-bottom: 15px;">
				<span class="methodology-tag">Empirical</span>
				<span class="methodology-tag">Multi-Turn Deception</span>
				<span class="methodology-tag">Conversational AI</span>
			</div>
			<div class="paper-links">
				<a href="https://arxiv.org/abs/2508.06457" class="paper-link" target="_blank">ğŸ“„ Paper</a>
			</div>
		</div>

		</div>

		<!-- July 2025 Papers Archive -->
		<div id="july-archive" class="section-card" style="margin-top: 50px; border: 2px solid #e9ecef;">
			<div style="text-align: center; margin-bottom: 30px;">
				<h2 style="color: #2c3e50; font-family: 'Times New Roman', 'Times', serif; margin-bottom: 10px;">ğŸ“š July 2025 Archive</h2>
				<p style="color: #6c757d; font-style: italic;">Previous month's top papers in AI deception research</p>
			</div>
			
			<!-- July Reading Recommendations Card -->
			<div class="section-card reading-guide" style="background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border: 2px solid #dee2e6; margin: 30px 0; cursor: pointer;" onclick="toggleJulyReadingGuide()">
				<div style="display: flex; align-items: center; justify-content: space-between;">
					<h2 style="color: #2c3e50; margin: 0; font-family: 'Times New Roman', 'Times', serif; font-size: 1.5em;">ğŸ“š July Reading Recommendations</h2>
					<span id="july-guide-toggle" style="font-size: 1.5em; color: #6c757d; transition: transform 0.3s ease;">â–¼</span>
				</div>
				<p style="margin: 10px 0 0 0; color: #495057;">Click to view our curated reading guide for July 2025's latest AI deception research</p>
				
				<div id="july-reading-guide-content" style="display: none; margin-top: 20px; border-top: 1px solid #dee2e6; padding-top: 20px;">
					<div style="background: white; border-radius: 8px; padding: 20px; margin-bottom: 20px; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
						<h3 style="color: #B22222; margin-bottom: 15px; font-family: 'Times New Roman', 'Times', serif;">ğŸ¯ Recommended Reading Path</h3>
						<div style="display: grid; gap: 15px;">
							<div style="border-left: 4px solid #B22222; padding-left: 15px;">
								<strong style="color: #2c3e50;">Foundation (Papers #1-2)</strong>
								<p style="margin: 5px 0; color: #495057; font-size: 0.95em;">Start with <em>"Manipulation Attacks"</em> and <em>"Strategic Phrasing"</em> to understand the current threat landscape and subtle deception techniques.</p>
							</div>
							<div style="border-left: 4px solid #DC143C; padding-left: 15px;">
								<strong style="color: #2c3e50;">Detection Methods (Papers #3-4, #6)</strong>
								<p style="margin: 5px 0; color: #495057; font-size: 0.95em;">Learn cutting-edge detection techniques: <em>activation patching</em>, <em>deception probes</em>, and <em>representation analysis</em>.</p>
							</div>
							<div style="border-left: 4px solid #FF6B6B; padding-left: 15px;">
								<strong style="color: #2c3e50;">Evaluation & Measurement (Paper #5)</strong>
								<p style="margin: 5px 0; color: #495057; font-size: 0.95em;">Understand how to measure AI deception through the innovative <em>"Bullshit Index"</em> framework.</p>
							</div>
							<div style="border-left: 4px solid #4ECDC4; padding-left: 15px;">
								<strong style="color: #2c3e50;">Emerging Threats (Paper #7)</strong>
								<p style="margin: 5px 0; color: #495057; font-size: 0.95em;">Explore steganographic capabilities in frontier models and hidden communication channels.</p>
							</div>
							<div style="border-left: 4px solid #45B7D1; padding-left: 15px;">
								<strong style="color: #2c3e50;">Mitigation Strategies (Papers #8-9)</strong>
								<p style="margin: 5px 0; color: #495057; font-size: 0.95em;">Study defense approaches: <em>CONSENSAGENT</em> for sycophancy mitigation and <em>ICLShield</em> for backdoor defense.</p>
							</div>
							<div style="border-left: 4px solid #96CEB4; padding-left: 15px;">
								<strong style="color: #2c3e50;">Advanced Applications (Paper #10)</strong>
								<p style="margin: 5px 0; color: #495057; font-size: 0.95em;">Examine deception in multi-agent economic scenarios and collusive behaviors.</p>
							</div>
						</div>
					</div>
					
					<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-bottom: 20px;">
						<div style="background: white; border-radius: 8px; padding: 15px; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
							<h4 style="color: #B22222; margin-bottom: 10px;">ğŸ” Research Categories</h4>
							<ul style="margin: 0; padding-left: 20px;">
								<li><strong>Detection & Analysis:</strong> Papers #3, #4, #6</li>
								<li><strong>Mitigation & Defense:</strong> Papers #8, #9</li>
								<li><strong>Empirical Studies:</strong> Papers #2, #5, #7, #10</li>
								<li><strong>Theoretical Frameworks:</strong> Paper #1</li>
							</ul>
						</div>
						<div style="background: white; border-radius: 8px; padding: 15px; box-shadow: 0 2px 4px rgba(0,0,0,0.05);">
							<h4 style="color: #B22222; margin-bottom: 10px;">ğŸ’¡ Study Tips</h4>
							<ul style="margin: 0; padding-left: 20px;">
								<li>Start with abstracts to gauge relevance</li>
								<li>Focus on methodology sections for techniques</li>
								<li>Pay attention to evaluation metrics</li>
								<li>Note limitations and future work</li>
							</ul>
						</div>
					</div>
					
					<div style="background: linear-gradient(45deg, #f8f9fa, #e9ecef); border-radius: 8px; padding: 15px; text-align: center;">
						<p style="margin: 0; color: #495057; font-style: italic;">
							ğŸŒŸ All papers are from <strong>July 2025</strong>, providing foundational research in AI deception.
							<br>Complement your reading with our <a href="tutorials.html" style="color: #B22222;">tutorials</a> and <a href="overview.html" style="color: #B22222;">overview</a> sections.
						</p>
					</div>
				</div>
			</div>
			
			<!-- July Papers 1-3 -->
			<div class="paper-card">
				<div class="paper-rank">1</div>
				<div class="impact-score">ğŸ“… July 2025</div>
				<div style="text-align: center; margin: 20px 40px;">
					<img src="./top10papers/july/images/Manipulation Attacks by Misaligned AI.png" alt="Manipulation Attacks by Misaligned AI" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
				</div>
				<div class="paper-title">Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework</div>
				<div class="paper-authors">Rishane Dassanayake et al.</div>
				<div class="paper-venue">July 18, 2025</div>
				<div class="paper-abstract">
					Examines the risk of manipulation attacks from LLM-based agents, including cases where AI systems strategically deceive humans to remove safeguards. Proposes a structured safety framework: proving inability, enforcing control, and ensuring trustworthiness to defend against strategic AI-driven deception.
				</div>
				<div style="margin-left: 40px; margin-bottom: 15px;">
					<span class="methodology-tag">Theoretical</span>
					<span class="methodology-tag">Safety Framework</span>
					<span class="methodology-tag">Risk Analysis</span>
				</div>
				<div class="paper-links">
					<a href="https://arxiv.org/abs/2507.12872" class="paper-link" target="_blank">ğŸ“„ Paper</a>
				</div>
			</div>

			<div class="paper-card">
				<div class="paper-rank">2</div>
				<div class="impact-score">ğŸ“… July 2025</div>
				<div style="text-align: center; margin: 20px 40px;">
					<img src="./top10papers/july/images/Language Models can Subtly Deceive Without Lying.png" alt="Language Models can Subtly Deceive Without Lying" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
				</div>
				<div class="paper-title">Language Models Can Subtly Deceive Without Lying: A Case Study on Strategic Phrasing in Legislation</div>
				<div class="paper-authors">Samyak Dogra et al.</div>
				<div class="paper-venue">ACL 2025</div>
				<div class="paper-abstract">
					Investigates how LLMs use subtle, strategic phrasing to influence legislative decision-making without producing outright falsehoods. Finds that deception success rates increase by up to 40% when phrasing strategies are optimized.
				</div>
				<div style="margin-left: 40px; margin-bottom: 15px;">
					<span class="methodology-tag">Empirical</span>
					<span class="methodology-tag">Strategic Deception</span>
					<span class="methodology-tag">Linguistic Manipulation</span>
				</div>
				<div class="paper-links">
					<a href="https://aclanthology.org/2025.acl-long.1600/" class="paper-link" target="_blank">ğŸ“„ Paper</a>
				</div>
			</div>

			<div class="paper-card">
				<div class="paper-rank">3</div>
				<div class="impact-score">ğŸ“… July 2025</div>
				<div style="text-align: center; margin: 20px 40px;">
					<img src="./top10papers/july/images/Adversarial Activation Patching.png" alt="Adversarial Activation Patching" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
				</div>
				<div class="paper-title">Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers</div>
				<div class="paper-authors">Santhosh Kumar Ravindran</div>
				<div class="paper-venue">July 14, 2025</div>
				<div class="paper-abstract">
					Uses activation patching to identify and induce deceptive behavior in RLHF-trained models. Pinpoints sparse, deception-related neurons and layers, enabling targeted mitigation strategies by detecting anomalous activations or fine-tuning on patched datasets.
				</div>
				<div style="margin-left: 40px; margin-bottom: 15px;">
					<span class="methodology-tag">Detection</span>
					<span class="methodology-tag">Mitigation</span>
					<span class="methodology-tag">Activation Patching</span>
				</div>
				<div class="paper-links">
					<a href="https://arxiv.org/abs/2507.09406" class="paper-link" target="_blank">ğŸ“„ Paper</a>
				</div>
			</div>

			<!-- July Papers 4-6 -->
			<div class="paper-card">
				<div class="paper-rank">4</div>
				<div class="impact-score">ğŸ“… July 2025</div>
				<div style="text-align: center; margin: 20px 40px;">
					<img src="./top10papers/july/images/Benchmarking Deception Probes.png" alt="Benchmarking Deception Probes" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
				</div>
				<div class="paper-title">Benchmarking Deception Probes via Black-to-White Performance Boosts</div>
				<div class="paper-authors">Avi Parrack, Carlo L. Attubato, Stefan Heimersheim</div>
				<div class="paper-venue">July 12, 2025</div>
				<div class="paper-abstract">
					Evaluates "deception probes" trained on LLM hidden activations to distinguish lies from truthful statements. Finds white-box probes outperform black-box detectors but only modestly, suggesting that current detection approaches remain fragile against adversarially deceptive models.
				</div>
				<div style="margin-left: 40px; margin-bottom: 15px;">
					<span class="methodology-tag">Detection</span>
					<span class="methodology-tag">Benchmarking</span>
					<span class="methodology-tag">Empirical</span>
				</div>
				<div class="paper-links">
					<a href="https://arxiv.org/abs/2507.12691" class="paper-link" target="_blank">ğŸ“„ Paper</a>
				</div>
			</div>

			<div class="paper-card">
				<div class="paper-rank">5</div>
				<div class="impact-score">ğŸ“… July 2025</div>
				<div style="text-align: center; margin: 20px 40px;">
					<img src="./top10papers/july/images/Machine Bullshit.png" alt="Machine Bullshit" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
				</div>
				<div class="paper-title">Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models</div>
				<div class="paper-authors">Liang et al.</div>
				<div class="paper-venue">July 10, 2025</div>
				<div class="paper-abstract">
					Introduces a Bullshit Index, a metric to measure LLMs' indifference to truth. Identifies four forms: empty rhetoric, paltering, weasel words, and unverified claims. Finds RLHF and chain-of-thought prompting often exacerbate these behaviors.
				</div>
				<div style="margin-left: 40px; margin-bottom: 15px;">
					<span class="methodology-tag">Empirical</span>
					<span class="methodology-tag">Evaluation Metrics</span>
					<span class="methodology-tag">Truth Indifference</span>
				</div>
				<div class="paper-links">
					<a href="https://arxiv.org/abs/2507.07484" class="paper-link" target="_blank">ğŸ“„ Paper</a>
				</div>
			</div>

			<div class="paper-card">
				<div class="paper-rank">6</div>
				<div class="impact-score">ğŸ“… July 2025</div>
				<div style="text-align: center; margin: 20px 40px;">
					<img src="./top10papers/july/images/When Truthful Representations Flip Under Deceptive Instructions.png" alt="When Truthful Representations Flip Under Deceptive Instructions" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
				</div>
				<div class="paper-title">When Truthful Representations Flip Under Deceptive Instructions?</div>
				<div class="paper-authors">Xianxuan Long et al.</div>
				<div class="paper-venue">July 10, 2025</div>
				<div class="paper-abstract">
					Analyzes how hidden representations in transformer models change under deceptive instructions. Finds distinct latent patterns in middle layers when prompted to lie, suggesting potential for early detection of deceptive outputs using linear probes on activations.
				</div>
				<div style="margin-left: 40px; margin-bottom: 15px;">
					<span class="methodology-tag">Detection</span>
					<span class="methodology-tag">Representation Analysis</span>
					<span class="methodology-tag">Interpretability</span>
				</div>
				<div class="paper-links">
					<a href="https://arxiv.org/abs/2507.22149" class="paper-link" target="_blank">ğŸ“„ Paper</a>
				</div>
			</div>

			<!-- July Papers 7-10 -->
			<div class="paper-card">
				<div class="paper-rank">7</div>
				<div class="impact-score">ğŸ“… July 2025</div>
				<div style="text-align: center; margin: 20px 40px;">
					<img src="./top10papers/july/images/Early Signs of Steganographic Capabilities in Frontier LLMs.png" alt="Early Signs of Steganographic Capabilities in Frontier LLMs" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
				</div>
				<div class="paper-title">Early Signs of Steganographic Capabilities in Frontier LLMs</div>
				<div class="paper-authors">Artur Zolkowski et al.</div>
				<div class="paper-venue">July 3, 2025</div>
				<div class="paper-abstract">
					Evaluates steganography capabilities in frontier LLMs, focusing on how models could evade monitoring through encoding hidden information within seemingly benign generations. The study examines both passing encoded messages and performing encoded reasoning.
				</div>
				<div style="margin-left: 40px; margin-bottom: 15px;">
					<span class="methodology-tag">Empirical</span>
					<span class="methodology-tag">Steganography</span>
					<span class="methodology-tag">Hidden Communication</span>
				</div>
				<div class="paper-links">
					<a href="https://arxiv.org/abs/2507.02737" class="paper-link" target="_blank">ğŸ“„ Paper</a>
				</div>
			</div>

			<div class="paper-card">
				<div class="paper-rank">8</div>
				<div class="impact-score">ğŸ“… July 2025</div>
				<div style="text-align: center; margin: 20px 40px;">
					<img src="./top10papers/july/images/ConsentAgent.png" alt="CONSENSAGENT" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
				</div>
				<div class="paper-title">CONSENSAGENT: Efficient and Effective Consensus in Multi-Agent LLM Systems through Sycophancy Mitigation</div>
				<div class="paper-authors">Priya Pitre, Naren Ramakrishnan, Xuan Wang</div>
				<div class="paper-venue">ACL 2025 Findings</div>
				<div class="paper-abstract">
					Proposes CONSENSAGENT, a system for mitigating sycophancy in multi-agent LLM setups. Encourages controlled dissent among agents to avoid echo-chamber agreement, improving consensus accuracy on six reasoning benchmarks while lowering computational costs.
				</div>
				<div style="margin-left: 40px; margin-bottom: 15px;">
					<span class="methodology-tag">Mitigation</span>
					<span class="methodology-tag">Multi-Agent Systems</span>
					<span class="methodology-tag">Sycophancy</span>
				</div>
				<div class="paper-links">
					<a href="https://aclanthology.org/2025.findings-acl.1123/" class="paper-link" target="_blank">ğŸ“„ Paper</a>
				</div>
			</div>

			<div class="paper-card">
				<div class="paper-rank">9</div>
				<div class="impact-score">ğŸ“… July 2025</div>
				<div style="text-align: center; margin: 20px 40px;">
					<img src="./top10papers/july/images/ICLShield- Exploring and Mitigating In-Context Learning Backdoor Attacks.png" alt="ICLShield" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
				</div>
				<div class="paper-title">ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks</div>
				<div class="paper-authors">Zhiyao Ren et al.</div>
				<div class="paper-venue">July 2, 2025</div>
				<div class="paper-abstract">
					Addresses vulnerabilities in in-context learning where adversaries can manipulate LLM behaviors by poisoning demonstrations. Proposes the dual-learning hypothesis and introduces ICLShield defense mechanism.
				</div>
				<div style="margin-left: 40px; margin-bottom: 15px;">
					<span class="methodology-tag">Mitigation</span>
					<span class="methodology-tag">Security</span>
					<span class="methodology-tag">Backdoor Defense</span>
				</div>
				<div class="paper-links">
					<a href="https://arxiv.org/abs/2507.01321" class="paper-link" target="_blank">ğŸ“„ Paper</a>
				</div>
			</div>

			<div class="paper-card">
				<div class="paper-rank">10</div>
				<div class="impact-score">ğŸ“… July 2025</div>
				<div style="text-align: center; margin: 20px 40px;">
					<img src="./top10papers/july/images/Evaluating LLM Agent Collusion in Double Auctions.png" alt="Evaluating LLM Agent Collusion in Double Auctions" style="max-width: 100%; height: 200px; object-fit: cover; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
				</div>
				<div class="paper-title">Evaluating LLM Agent Collusion in Double Auctions</div>
				<div class="paper-authors">Kushal Agrawal et al.</div>
				<div class="paper-venue">July 2, 2025</div>
				<div class="paper-abstract">
					Examines scenarios where LLM agents can choose to collude (secretive cooperation that harms another party) in simulated continuous double auction markets, analyzing how communication, model choice, and environmental pressures affect collusive tendencies.
				</div>
				<div style="margin-left: 40px; margin-bottom: 15px;">
					<span class="methodology-tag">Empirical</span>
					<span class="methodology-tag">Agent Behavior</span>
					<span class="methodology-tag">Economic Simulation</span>
				</div>
				<div class="paper-links">
					<a href="https://arxiv.org/abs/2507.01413" class="paper-link" target="_blank">ğŸ“„ Paper</a>
				</div>
			</div>
		</div>

	</div>
</section>

<footer class="footer">
	<div class="content has-text-centered">
		<p style="color: #666; font-family: 'Times New Roman', 'Times', serif;">
			Â© 2025 PKU-Alignment Team. This website is adapted from <a href="https://nerfies.github.io/">Nerfies</a>, 
			licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
		</p>
	</div>
</footer>

</body>
</html>
